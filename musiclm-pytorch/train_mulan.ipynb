{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environment variable to specify which GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 15:05:36.389693: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-02 15:05:36.391351: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 15:05:36.427928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-02 15:05:37.393900: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from musiclm_pytorch import MuLaNTrainer, MuLaN\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transformer = AudioSpectrogramTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64,\n",
    "    spec_n_fft = 256,\n",
    "    spec_win_length = 24,\n",
    "    spec_aug_stretch_factor = 0.8\n",
    ")\n",
    "\n",
    "text_transformer = TextTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64\n",
    ")\n",
    "\n",
    "mulan = MuLaN(\n",
    "    audio_transformer = audio_transformer,\n",
    "    text_transformer = text_transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, musiccaps_dataset_pkl_path: Path, sdd_dataset_pkl_path: Path):\n",
    "                \n",
    "        self.musiccaps_dataset = pickle.load(open(musiccaps_dataset_pkl_path, 'rb'))\n",
    "        self.num_musiccaps = len(self.musiccaps_dataset)\n",
    "\n",
    "        self.sdd_dataset = pickle.load(open(sdd_dataset_pkl_path, 'rb'))\n",
    "        self.num_sdd = len(self.sdd_dataset)\n",
    "     \n",
    "        self.num_data = self.num_musiccaps + self.num_sdd\n",
    "        \n",
    "        self.wav_duration = 16000 * 10 # 10 seconds\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.num_musiccaps:\n",
    "            wav = self.musiccaps_dataset[idx][0]\n",
    "            cap = self.musiccaps_dataset[idx][1]\n",
    "        \n",
    "        else:\n",
    "            # get the sdd_dataset_index\n",
    "            idx = idx - self.num_musiccaps\n",
    "            real_wav_len = self.sdd_dataset[idx][2]\n",
    "            \n",
    "            # randomly select a starting point\n",
    "            start_point = torch.randint(0, real_wav_len - self.wav_duration, (1,)).item()\n",
    "            wav = self.sdd_dataset[idx][0][start_point:start_point+self.wav_duration]\n",
    "            cap = self.sdd_dataset[idx][1]\n",
    "            \n",
    "        return wav, cap\n",
    "    \n",
    "    \n",
    "# training_data = MuLanDataset(\n",
    "#     txt_pickle_path=Path('pkls/txts.pkl'),\n",
    "#     wav_pickle_path=Path('pkls/wavs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires ~ 45GB of DRAM\n",
    "training_data = MusicDataset(\n",
    "    musiccaps_dataset_pkl_path=Path('pkls/musiccaps_dataset.pkl'),\n",
    "    sdd_dataset_pkl_path=Path('pkls/sdd_dataset.pkl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5480\n",
      "torch.Size([160000]) Rainy piano vibe well mix and spatially pan vocal\n",
      "torch.Size([160000]) this record contain break and shoot sound . there be also a lot of deep rumble noise . the whole audio be pan to the right side of the speaker . this be an amateur record and of poor audio quality . this audio may be play in a video game .\n"
     ]
    }
   ],
   "source": [
    "# test MusicDataset\n",
    "num_musiccap_data = training_data.num_musiccaps\n",
    "print(num_musiccap_data)\n",
    "wav, txt = training_data[num_musiccap_data + 10]\n",
    "print(wav.shape, txt)\n",
    "\n",
    "wav, txt = training_data[10]\n",
    "print(wav.shape, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6186\n"
     ]
    }
   ],
   "source": [
    "print(training_data.num_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Mulan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 5876 samples and validating with randomly splitted 310 samples\n"
     ]
    }
   ],
   "source": [
    "mulan_trainer = MuLaNTrainer(mulan=mulan, dataset=training_data, num_train_steps=2000, batch_size=24, grad_accum_every=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectrogram yielded shape of (129, 1251), but had to be cropped to (128, 1248) to be patchified for transformer\n",
      "0: loss: 3.146928042173385\n",
      "0: saving model to results\n",
      "0: saving model with minimum loss to results\n",
      "1: loss: 3.5955953995386754\n",
      "2: loss: 3.1361465950806937\n",
      "2: saving model with minimum loss to results\n",
      "3: loss: 3.135649800300598\n",
      "3: saving model with minimum loss to results\n",
      "4: loss: 3.129435191551844\n",
      "4: saving model with minimum loss to results\n",
      "5: loss: 3.1293199757734933\n",
      "5: saving model with minimum loss to results\n",
      "6: loss: 3.1254666348298397\n",
      "6: saving model with minimum loss to results\n",
      "7: loss: 3.1177236338456473\n",
      "7: saving model with minimum loss to results\n",
      "8: loss: 3.1072869002819066\n",
      "8: saving model with minimum loss to results\n",
      "9: loss: 3.091572354237239\n",
      "9: saving model with minimum loss to results\n",
      "10: loss: 3.0837446848551435\n",
      "10: saving model with minimum loss to results\n",
      "11: loss: 3.0354432463645935\n",
      "11: saving model with minimum loss to results\n",
      "12: loss: 3.0267563958962755\n",
      "12: saving model with minimum loss to results\n",
      "13: loss: 3.0774093767007193\n",
      "14: loss: 2.993159343798955\n",
      "14: saving model with minimum loss to results\n",
      "15: loss: 3.067707439263662\n",
      "16: loss: 3.0983439584573116\n",
      "17: loss: 2.965854416290919\n",
      "17: saving model with minimum loss to results\n",
      "18: loss: 2.9980215827624\n",
      "19: loss: 2.9425868193308506\n",
      "19: saving model with minimum loss to results\n",
      "20: loss: 3.0301509598890948\n",
      "21: loss: 3.0083454449971514\n",
      "22: loss: 2.8842955032984414\n",
      "22: saving model with minimum loss to results\n",
      "23: loss: 2.9521471758683524\n",
      "24: loss: 2.9348031183083862\n",
      "25: loss: 2.849074761072795\n",
      "25: saving model with minimum loss to results\n",
      "26: loss: 2.9310896595319114\n",
      "27: loss: 2.9242709279060364\n",
      "28: loss: 2.8368502954641976\n",
      "28: saving model with minimum loss to results\n",
      "29: loss: 2.8954447706540423\n",
      "30: loss: 2.9729179143905635\n",
      "31: loss: 2.794182519117991\n",
      "31: saving model with minimum loss to results\n",
      "32: loss: 2.919064988692602\n",
      "33: loss: 2.9631202121575675\n",
      "34: loss: 2.8351784646511073\n",
      "35: loss: 2.7560249666372933\n",
      "35: saving model with minimum loss to results\n",
      "36: loss: 2.8233622809251155\n",
      "37: loss: 2.780219395955404\n",
      "38: loss: 2.630484839280446\n",
      "38: saving model with minimum loss to results\n",
      "39: loss: 2.6964396933714547\n",
      "40: loss: 2.6237917741139736\n",
      "40: saving model with minimum loss to results\n",
      "41: loss: 2.7430732548236856\n",
      "42: loss: 2.7588965495427447\n",
      "43: loss: 2.6190492312113443\n",
      "43: saving model with minimum loss to results\n",
      "44: loss: 2.7043375770250955\n",
      "45: loss: 2.5722692906856537\n",
      "45: saving model with minimum loss to results\n",
      "46: loss: 2.54004243016243\n",
      "46: saving model with minimum loss to results\n",
      "47: loss: 2.5811551411946607\n",
      "48: loss: 2.478100687265396\n",
      "48: saving model with minimum loss to results\n",
      "49: loss: 2.4520474374294285\n",
      "49: saving model with minimum loss to results\n",
      "50: loss: 2.46914001305898\n",
      "51: loss: 2.4303054710229235\n",
      "51: saving model with minimum loss to results\n",
      "52: loss: 2.374779423077901\n",
      "52: saving model with minimum loss to results\n",
      "53: loss: 2.397721389929453\n",
      "54: loss: 2.401395673553149\n",
      "55: loss: 2.4212590505679445\n",
      "56: loss: 2.3341746777296066\n",
      "56: saving model with minimum loss to results\n",
      "57: loss: 2.335105523467064\n",
      "58: loss: 2.413859476645788\n",
      "59: loss: 2.2955764184395475\n",
      "59: saving model with minimum loss to results\n",
      "60: loss: 2.411341557900111\n",
      "61: loss: 2.3259544670581818\n",
      "62: loss: 2.14062858124574\n",
      "62: saving model with minimum loss to results\n",
      "63: loss: 2.1244902660449347\n",
      "63: saving model with minimum loss to results\n",
      "64: loss: 2.1818664421637854\n",
      "65: loss: 2.177212630709012\n",
      "66: loss: 2.19833853840828\n",
      "67: loss: 2.212145507335663\n",
      "68: loss: 2.2130062033732734\n",
      "69: loss: 2.1305686235427856\n",
      "70: loss: 2.114068791270256\n",
      "70: saving model with minimum loss to results\n",
      "71: loss: 1.9778891056776051\n",
      "71: saving model with minimum loss to results\n",
      "72: loss: 1.9704557061195374\n",
      "72: saving model with minimum loss to results\n",
      "73: loss: 2.006431465347608\n",
      "74: loss: 1.9954623182614648\n",
      "75: loss: 1.833736300468445\n",
      "75: saving model with minimum loss to results\n",
      "76: loss: 1.9303559462229414\n",
      "77: loss: 1.9453547249237695\n",
      "78: loss: 1.909645626942317\n",
      "79: loss: 2.1319248328606295\n",
      "80: loss: 2.012995431820552\n",
      "81: loss: 2.11237545311451\n",
      "82: loss: 1.997124617298444\n",
      "83: loss: 1.849388465285301\n",
      "84: loss: 1.8795021573702495\n",
      "85: loss: 1.7661369989315672\n",
      "85: saving model with minimum loss to results\n",
      "86: loss: 1.8086171597242353\n",
      "87: loss: 1.7041325022776923\n",
      "87: saving model with minimum loss to results\n",
      "88: loss: 1.6249652802944181\n",
      "88: saving model with minimum loss to results\n",
      "89: loss: 1.6770421117544174\n",
      "90: loss: 1.764486993352572\n",
      "91: loss: 1.5245022227366765\n",
      "91: saving model with minimum loss to results\n",
      "92: loss: 1.5312874813874566\n",
      "93: loss: 1.4354899724324546\n",
      "93: saving model with minimum loss to results\n",
      "94: loss: 1.7980290551980334\n",
      "95: loss: 1.6479385743538535\n",
      "96: loss: 1.5789560079574583\n",
      "97: loss: 1.5572543293237688\n",
      "98: loss: 1.472492203116417\n",
      "99: loss: 1.4807784135142963\n",
      "100: loss: 1.5436211377382278\n",
      "101: loss: 1.2916591713825862\n",
      "101: saving model with minimum loss to results\n",
      "102: loss: 1.3674043292800586\n",
      "103: loss: 1.3964437196652095\n",
      "104: loss: 1.3298705890774725\n",
      "105: loss: 1.4557976921399431\n",
      "106: loss: 1.4524975369373958\n",
      "107: loss: 1.0735711033145587\n",
      "107: saving model with minimum loss to results\n",
      "108: loss: 1.6135847171147666\n",
      "109: loss: 1.397450104355812\n",
      "110: loss: 1.459343433380127\n",
      "111: loss: 1.632727260390917\n",
      "112: loss: 1.1818465242783227\n",
      "113: loss: 1.2847765957315762\n",
      "114: loss: 1.1578412850697835\n",
      "115: loss: 1.0004527320464454\n",
      "115: saving model with minimum loss to results\n",
      "116: loss: 1.2195550923546157\n",
      "117: loss: 1.0829822942614555\n",
      "118: loss: 1.1361468459169066\n",
      "119: loss: 1.1186311741669972\n",
      "120: loss: 1.1877306799093879\n",
      "121: loss: 1.002917731801669\n",
      "122: loss: 1.1408548578619957\n",
      "123: loss: 1.2481708228588104\n",
      "124: loss: 0.8578797914087773\n",
      "124: saving model with minimum loss to results\n",
      "125: loss: 1.2214891786376636\n",
      "126: loss: 1.0111914550264676\n",
      "127: loss: 0.9527370656530064\n",
      "128: loss: 0.8734484389424324\n",
      "129: loss: 0.8507716034849484\n",
      "129: saving model with minimum loss to results\n",
      "130: loss: 1.0275214165449142\n",
      "131: loss: 0.8578006885945796\n",
      "132: loss: 0.8280583458642163\n",
      "132: saving model with minimum loss to results\n",
      "133: loss: 0.9177764045695463\n",
      "134: loss: 0.6955690632263818\n",
      "134: saving model with minimum loss to results\n",
      "135: loss: 0.7433683586617311\n",
      "136: loss: 0.7751496769487859\n",
      "137: loss: 0.724297241618236\n",
      "138: loss: 0.8540095649659634\n",
      "139: loss: 0.7316913679242133\n",
      "140: loss: 0.6809496767818928\n",
      "140: saving model with minimum loss to results\n",
      "141: loss: 0.8015464668472606\n",
      "142: loss: 0.5491236820816994\n",
      "142: saving model with minimum loss to results\n",
      "143: loss: 0.4813370835036038\n",
      "143: saving model with minimum loss to results\n",
      "144: loss: 0.5552884178857008\n",
      "145: loss: 0.5410264829794565\n",
      "146: loss: 0.4794694265971581\n",
      "146: saving model with minimum loss to results\n",
      "147: loss: 0.4374641304214796\n",
      "147: saving model with minimum loss to results\n",
      "148: loss: 0.7015888753036659\n",
      "149: loss: 0.578961664189895\n",
      "150: loss: 0.7123330856362979\n",
      "151: loss: 0.561250655601422\n",
      "152: loss: 0.7542784909407299\n",
      "153: loss: 0.6062088732918104\n",
      "154: loss: 0.447270671526591\n",
      "155: loss: 0.5952353353301684\n",
      "156: loss: 0.6771660161515078\n",
      "157: loss: 0.6506478227674961\n",
      "158: loss: 0.6380643161634606\n",
      "159: loss: 0.582959083840251\n",
      "160: loss: 0.5718379281461239\n",
      "161: loss: 0.48543088324368\n",
      "162: loss: 0.4690412065635125\n",
      "163: loss: 0.4463339075446129\n",
      "164: loss: 0.48564336821436876\n",
      "165: loss: 0.5950757053991159\n",
      "166: loss: 0.4214712971200546\n",
      "166: saving model with minimum loss to results\n",
      "167: loss: 0.7053092221419017\n",
      "168: loss: 0.5065388443569343\n",
      "169: loss: 0.6381189885238806\n",
      "170: loss: 0.43924603300789994\n",
      "171: loss: 0.5724213775247337\n",
      "172: loss: 0.45797177342077094\n",
      "173: loss: 0.3464468882108728\n",
      "173: saving model with minimum loss to results\n",
      "174: loss: 0.40784810421367484\n",
      "175: loss: 0.23949172495243454\n",
      "175: saving model with minimum loss to results\n",
      "176: loss: 0.4138027812975148\n",
      "177: loss: 0.4813172090798616\n",
      "178: loss: 0.27763737676044303\n",
      "179: loss: 0.3473123980996509\n",
      "180: loss: 0.42684426220754784\n",
      "181: loss: 0.5059320386499166\n",
      "182: loss: 0.3721933405225476\n",
      "183: loss: 0.3698765278483431\n",
      "184: loss: 0.2574644582346082\n",
      "185: loss: 0.3853270900435745\n",
      "186: loss: 0.26435438605646294\n",
      "187: loss: 0.425302146623532\n",
      "188: loss: 0.31254801790540415\n",
      "189: loss: 0.4070592564530672\n",
      "190: loss: 0.38859379664063454\n",
      "191: loss: 0.4010413340292871\n",
      "192: loss: 0.3069106160352628\n",
      "193: loss: 0.23855412347863114\n",
      "193: saving model with minimum loss to results\n",
      "194: loss: 0.23252513973663252\n",
      "194: saving model with minimum loss to results\n",
      "195: loss: 0.26205734862014657\n",
      "196: loss: 0.20115021414433915\n",
      "196: saving model with minimum loss to results\n",
      "197: loss: 0.40218650735914707\n",
      "198: loss: 0.25158568564802414\n",
      "199: loss: 0.29085863381624216\n",
      "200: loss: 0.3663892379651468\n",
      "201: loss: 0.2913705593285461\n",
      "202: loss: 0.3147860553581268\n",
      "203: loss: 0.15734266319001713\n",
      "203: saving model with minimum loss to results\n",
      "204: loss: 0.2503246758133173\n",
      "205: loss: 0.21664924593642348\n",
      "206: loss: 0.2576329646011194\n",
      "207: loss: 0.29505395082136\n",
      "208: loss: 0.2431168892265608\n",
      "209: loss: 0.27532038775583106\n",
      "210: loss: 0.3623499243209759\n",
      "211: loss: 0.2858169569323461\n",
      "212: loss: 0.3238614813114206\n",
      "213: loss: 0.22955134107420833\n",
      "214: loss: 0.3802353336165349\n",
      "215: loss: 0.30418558589493233\n",
      "216: loss: 0.38491958302135276\n",
      "217: loss: 0.29406299131611985\n",
      "218: loss: 0.2814181315091749\n",
      "219: loss: 0.2653137470285098\n",
      "220: loss: 0.20313521490121877\n",
      "221: loss: 0.30112847996254766\n",
      "222: loss: 0.198555807583034\n",
      "223: loss: 0.305279427052786\n",
      "224: loss: 0.2168093137443066\n",
      "225: loss: 0.19990213634446263\n",
      "226: loss: 0.2320256571595867\n",
      "227: loss: 0.3204903014314671\n",
      "228: loss: 0.2620070166885853\n",
      "229: loss: 0.2577661420218646\n",
      "230: loss: 0.3538282403023914\n",
      "231: loss: 0.36189428716897953\n",
      "232: loss: 0.21142649118943763\n",
      "233: loss: 0.16990694520063698\n",
      "234: loss: 0.2834824113330493\n",
      "235: loss: 0.206702685294052\n",
      "236: loss: 0.14409814367536458\n",
      "236: saving model with minimum loss to results\n",
      "237: loss: 0.15015649143606424\n",
      "238: loss: 0.2035860831771667\n",
      "239: loss: 0.26818194193765527\n",
      "240: loss: 0.22357776388525963\n",
      "241: loss: 0.17062056212065121\n",
      "242: loss: 0.1709320469138523\n",
      "243: loss: 0.2138130674138665\n",
      "244: loss: 0.1567899035289883\n",
      "245: loss: 0.1823016668204218\n",
      "246: loss: 0.314136372685122\n",
      "247: loss: 0.11995588088757357\n",
      "247: saving model with minimum loss to results\n",
      "248: loss: 0.1416094528200726\n",
      "249: loss: 0.19141055168195942\n",
      "250: loss: 0.11574049138774475\n",
      "250: saving model with minimum loss to results\n",
      "251: loss: 0.2378782652473698\n",
      "252: loss: 0.21988384014305967\n",
      "253: loss: 0.17574205637599033\n",
      "254: loss: 0.13884899524661404\n",
      "255: loss: 0.18700003864554066\n",
      "256: loss: 0.153260859195143\n",
      "257: loss: 0.12593292394497743\n",
      "258: loss: 0.11369505237477523\n",
      "258: saving model with minimum loss to results\n",
      "259: loss: 0.23965097286660847\n",
      "260: loss: 0.1691583656550695\n",
      "261: loss: 0.21851964015513659\n",
      "262: loss: 0.1634233539613585\n",
      "263: loss: 0.21215731091797352\n",
      "264: loss: 0.09555377357173711\n",
      "264: saving model with minimum loss to results\n",
      "265: loss: 0.1492864340543747\n",
      "266: loss: 0.18831730406964198\n",
      "267: loss: 0.20095777611519833\n",
      "268: loss: 0.14681406482122838\n",
      "269: loss: 0.21081964636687192\n",
      "270: loss: 0.16853758650055775\n",
      "271: loss: 0.24945070497536415\n",
      "272: loss: 0.13910979653398195\n",
      "273: loss: 0.1561380825781574\n",
      "274: loss: 0.133515383504952\n",
      "275: loss: 0.09204928432397233\n",
      "275: saving model with minimum loss to results\n",
      "276: loss: 0.09541445504873992\n",
      "277: loss: 0.1027342483672934\n",
      "278: loss: 0.15337234945036476\n",
      "279: loss: 0.13040956909147403\n",
      "280: loss: 0.175712182186544\n",
      "281: loss: 0.17929158021191446\n",
      "282: loss: 0.17136822007402466\n",
      "283: loss: 0.14246297427841154\n",
      "284: loss: 0.2200033930130303\n",
      "285: loss: 0.13780980997641257\n",
      "286: loss: 0.12309663375587358\n",
      "287: loss: 0.10962430857277164\n",
      "288: loss: 0.2442587886859353\n",
      "289: loss: 0.17190954585870108\n",
      "290: loss: 0.15126845364769298\n",
      "291: loss: 0.11673345696181057\n",
      "292: loss: 0.12761434610001743\n",
      "293: loss: 0.14272472076117992\n",
      "294: loss: 0.31831436937985325\n",
      "295: loss: 0.13111250511428807\n",
      "296: loss: 0.15513189773385724\n",
      "297: loss: 0.09703209581008802\n",
      "298: loss: 0.13217059445256987\n",
      "299: loss: 0.17830114544873746\n",
      "300: loss: 0.09893356569227765\n",
      "301: loss: 0.16690048702488033\n",
      "302: loss: 0.21939412372497227\n",
      "303: loss: 0.13813515178238353\n",
      "304: loss: 0.10513224879590174\n",
      "305: loss: 0.1412912278901786\n",
      "306: loss: 0.13586534806139147\n",
      "307: loss: 0.15681827991890407\n",
      "308: loss: 0.12240154219519658\n",
      "309: loss: 0.14355771575355902\n",
      "310: loss: 0.11834600288420913\n",
      "311: loss: 0.13234086513208845\n",
      "312: loss: 0.15167964537007111\n",
      "313: loss: 0.08991303927420329\n",
      "313: saving model with minimum loss to results\n",
      "314: loss: 0.11930491714156234\n",
      "315: loss: 0.10204239728045648\n",
      "316: loss: 0.18025652596649405\n",
      "317: loss: 0.06858396235232551\n",
      "317: saving model with minimum loss to results\n",
      "318: loss: 0.1084026403259486\n",
      "319: loss: 0.14251856150804085\n",
      "320: loss: 0.14155936365326247\n",
      "321: loss: 0.1179613188141957\n",
      "322: loss: 0.1877488281267385\n",
      "323: loss: 0.14876224809510555\n",
      "324: loss: 0.13756551764284572\n",
      "325: loss: 0.1541041460974763\n",
      "326: loss: 0.10832570422098799\n",
      "327: loss: 0.13199920432331663\n",
      "328: loss: 0.09912580823826525\n",
      "329: loss: 0.1309411081795891\n",
      "330: loss: 0.14692879122837138\n",
      "331: loss: 0.12114373672132689\n",
      "332: loss: 0.15786414252943357\n",
      "333: loss: 0.14565325894121398\n",
      "334: loss: 0.15792088818852787\n",
      "335: loss: 0.07378036563750356\n",
      "336: loss: 0.14644697882855934\n",
      "337: loss: 0.14120363867429353\n",
      "338: loss: 0.1427840564865619\n",
      "339: loss: 0.0869736415334046\n",
      "340: loss: 0.18952796357916674\n",
      "341: loss: 0.16540323481118926\n",
      "342: loss: 0.14812820373723903\n",
      "343: loss: 0.10733641385256001\n",
      "344: loss: 0.13870110023223486\n",
      "345: loss: 0.10433797076499711\n",
      "346: loss: 0.17247635463718328\n",
      "347: loss: 0.12472754927390876\n",
      "348: loss: 0.10608819071785545\n",
      "349: loss: 0.113663893736278\n",
      "350: loss: 0.111762141201325\n",
      "351: loss: 0.13906445433773723\n",
      "352: loss: 0.13201904657762498\n",
      "353: loss: 0.1071726991115914\n",
      "354: loss: 0.14859812611636394\n",
      "355: loss: 0.10047779664637344\n",
      "356: loss: 0.09441486831444004\n",
      "357: loss: 0.10469032148830593\n",
      "358: loss: 0.11735902259048694\n",
      "359: loss: 0.11017790512414648\n",
      "360: loss: 0.0665058728288083\n",
      "360: saving model with minimum loss to results\n",
      "361: loss: 0.1718086140657154\n",
      "362: loss: 0.047893933913049594\n",
      "362: saving model with minimum loss to results\n",
      "363: loss: 0.12030771127319898\n",
      "364: loss: 0.09263203754865876\n",
      "365: loss: 0.06443908272679741\n",
      "366: loss: 0.06653890660527395\n",
      "367: loss: 0.13729075510582334\n",
      "368: loss: 0.07371204341567741\n",
      "369: loss: 0.12571345929366845\n",
      "370: loss: 0.12464965961407869\n",
      "371: loss: 0.12967565865740957\n",
      "372: loss: 0.07290821301285177\n",
      "373: loss: 0.18847352848388255\n",
      "374: loss: 0.11781795510614756\n",
      "375: loss: 0.08212766972913717\n",
      "376: loss: 0.05867277111004417\n",
      "377: loss: 0.0688542037969455\n",
      "378: loss: 0.13069533417001367\n",
      "379: loss: 0.09595140100767216\n",
      "380: loss: 0.11612823974185935\n",
      "381: loss: 0.11913277821925777\n",
      "382: loss: 0.12084688630420715\n",
      "383: loss: 0.13854691168914238\n",
      "384: loss: 0.12775958603742765\n",
      "385: loss: 0.21236357616726315\n",
      "386: loss: 0.04518551685032435\n",
      "386: saving model with minimum loss to results\n",
      "387: loss: 0.13044698557738835\n",
      "388: loss: 0.0758036936458666\n",
      "389: loss: 0.1416364352820286\n",
      "390: loss: 0.17430135204146305\n",
      "391: loss: 0.07419801900869061\n",
      "392: loss: 0.07357586135428089\n",
      "393: loss: 0.14103147123629847\n",
      "394: loss: 0.1496424762299284\n",
      "395: loss: 0.12053902307525276\n",
      "396: loss: 0.05678191283853569\n",
      "397: loss: 0.0758529016493412\n",
      "398: loss: 0.03418094298103825\n",
      "398: saving model with minimum loss to results\n",
      "399: loss: 0.0924534273702496\n",
      "400: loss: 0.12074317685134398\n",
      "401: loss: 0.08918994199484588\n",
      "402: loss: 0.06348759391888355\n",
      "403: loss: 0.07025060092564672\n",
      "404: loss: 0.11307322621966401\n",
      "405: loss: 0.14225251763127744\n",
      "406: loss: 0.08374512542892867\n",
      "407: loss: 0.06726897969686736\n",
      "408: loss: 0.08135195855478135\n",
      "409: loss: 0.09521519959283373\n",
      "410: loss: 0.08449421303036313\n",
      "411: loss: 0.15771752272848966\n",
      "412: loss: 0.05940251607292641\n",
      "413: loss: 0.09176632273010911\n",
      "414: loss: 0.08444939869999267\n",
      "415: loss: 0.05599397777890166\n",
      "416: loss: 0.11293722335540225\n",
      "417: loss: 0.11052962303316842\n",
      "418: loss: 0.06082451774273067\n",
      "419: loss: 0.05213941583254685\n",
      "420: loss: 0.10360154571632543\n",
      "421: loss: 0.12491978792240843\n",
      "422: loss: 0.06975646756472996\n",
      "423: loss: 0.12187002350886662\n",
      "424: loss: 0.12309307706012379\n",
      "425: loss: 0.09628950040011357\n",
      "426: loss: 0.08789788838475943\n",
      "427: loss: 0.04724768240218206\n",
      "428: loss: 0.06459267988490562\n",
      "429: loss: 0.1117260002841552\n",
      "430: loss: 0.15802076711164162\n",
      "431: loss: 0.08625217356408635\n",
      "432: loss: 0.11180365753049651\n",
      "433: loss: 0.06896894421273222\n",
      "434: loss: 0.13446427538292482\n",
      "435: loss: 0.1181155654291312\n",
      "436: loss: 0.09773974651276755\n",
      "437: loss: 0.09349832841689933\n",
      "438: loss: 0.08537426125258206\n",
      "439: loss: 0.0794416983978105\n",
      "440: loss: 0.07718768704216926\n",
      "441: loss: 0.0747951991991916\n",
      "442: loss: 0.059848086481603474\n",
      "443: loss: 0.08350326879493268\n",
      "444: loss: 0.08438722727199395\n",
      "445: loss: 0.0712915382658442\n",
      "446: loss: 0.11729412153363226\n",
      "447: loss: 0.14532484191780287\n",
      "448: loss: 0.04123379369654381\n",
      "449: loss: 0.10699049107885608\n",
      "450: loss: 0.1641920139663853\n",
      "451: loss: 0.07716582725212599\n",
      "452: loss: 0.04967133320557574\n",
      "453: loss: 0.12370732898125425\n",
      "454: loss: 0.07823780691251159\n",
      "455: loss: 0.16531817824579775\n",
      "456: loss: 0.06923516882428278\n",
      "457: loss: 0.10190879460424185\n",
      "458: loss: 0.0653304330771789\n",
      "459: loss: 0.11628917227305163\n",
      "460: loss: 0.07850608378794277\n",
      "461: loss: 0.07984869169498175\n",
      "462: loss: 0.13248439655823555\n",
      "463: loss: 0.0512679365541165\n",
      "464: loss: 0.07357929976812254\n",
      "465: loss: 0.045127599577729896\n",
      "466: loss: 0.12043602466140631\n",
      "467: loss: 0.15447942931981135\n",
      "468: loss: 0.05701754786423407\n",
      "469: loss: 0.07815628244134129\n",
      "470: loss: 0.09583805365158088\n",
      "471: loss: 0.0761929323077008\n",
      "472: loss: 0.09779611070795605\n",
      "473: loss: 0.06839670346380446\n",
      "474: loss: 0.09114218561444429\n",
      "475: loss: 0.057494806455603495\n",
      "476: loss: 0.06512105067182954\n",
      "477: loss: 0.10894194462647042\n",
      "478: loss: 0.03882544852482776\n",
      "479: loss: 0.0732552872505039\n",
      "480: loss: 0.0483263791150724\n",
      "481: loss: 0.08935013325147641\n",
      "482: loss: 0.05013975829630604\n",
      "483: loss: 0.08806166875486572\n",
      "484: loss: 0.06272005188899735\n",
      "485: loss: 0.04948476064782881\n",
      "486: loss: 0.08483680814970283\n",
      "487: loss: 0.06614020982912433\n",
      "488: loss: 0.029244759896149237\n",
      "488: saving model with minimum loss to results\n",
      "489: loss: 0.19355882005766034\n",
      "490: loss: 0.09351280917568752\n",
      "491: loss: 0.040793912577404015\n",
      "492: loss: 0.048691136374448746\n",
      "493: loss: 0.06111765933261874\n",
      "494: loss: 0.09737182292155923\n",
      "495: loss: 0.04327166174577239\n",
      "496: loss: 0.06639256608468713\n",
      "497: loss: 0.09968624759737092\n",
      "498: loss: 0.13944076195669672\n",
      "499: loss: 0.051340032926721804\n",
      "500: loss: 0.11034990565288658\n",
      "501: loss: 0.06284691473895994\n",
      "502: loss: 0.13544403856697804\n",
      "503: loss: 0.05638276842607108\n",
      "504: loss: 0.07114373582104844\n",
      "505: loss: 0.08236530540913616\n",
      "506: loss: 0.07578964723506942\n",
      "507: loss: 0.08982549182837828\n",
      "508: loss: 0.10829094198804037\n",
      "509: loss: 0.041438664377589404\n",
      "510: loss: 0.10276950304978527\n",
      "511: loss: 0.10063033235686211\n",
      "512: loss: 0.05801287597084107\n",
      "513: loss: 0.06394579332845751\n",
      "514: loss: 0.10444575389071055\n",
      "515: loss: 0.059553233176605616\n",
      "516: loss: 0.051965311305442206\n",
      "517: loss: 0.03248023723669273\n",
      "518: loss: 0.03346997546032072\n",
      "519: loss: 0.052760873223936265\n",
      "520: loss: 0.042327691798467036\n",
      "521: loss: 0.07938189385458827\n",
      "522: loss: 0.10634780184288202\n",
      "523: loss: 0.0983149082167074\n",
      "524: loss: 0.08521063577306148\n",
      "525: loss: 0.08088818077036801\n",
      "526: loss: 0.06327802150432642\n",
      "527: loss: 0.09346195401546235\n",
      "528: loss: 0.10028230123377092\n",
      "529: loss: 0.03353667082168007\n",
      "530: loss: 0.024426150649863605\n",
      "530: saving model with minimum loss to results\n",
      "531: loss: 0.10782687961667155\n",
      "532: loss: 0.03048064901183049\n",
      "533: loss: 0.06803193713130895\n",
      "534: loss: 0.036075514974072576\n",
      "535: loss: 0.03596993638833131\n",
      "536: loss: 0.07344572817479882\n",
      "537: loss: 0.06680789054371418\n",
      "538: loss: 0.10909837276752417\n",
      "539: loss: 0.13000265116958568\n",
      "540: loss: 0.03046362984847898\n",
      "541: loss: 0.10202251956798139\n",
      "542: loss: 0.0437216280940144\n",
      "543: loss: 0.09856963712566842\n",
      "544: loss: 0.07123788758569087\n",
      "545: loss: 0.10493951678654412\n",
      "546: loss: 0.05437115941458615\n",
      "547: loss: 0.07302292288901906\n",
      "548: loss: 0.06786578237855186\n",
      "549: loss: 0.05358562800878038\n",
      "550: loss: 0.08491871560302874\n",
      "551: loss: 0.06593478690289581\n",
      "552: loss: 0.08870469481917098\n",
      "553: loss: 0.04007317973749499\n",
      "554: loss: 0.05235073797909233\n",
      "555: loss: 0.06292433790319289\n",
      "556: loss: 0.03137401497224345\n",
      "557: loss: 0.06159172018912309\n",
      "558: loss: 0.06898291451701274\n",
      "559: loss: 0.09185309453459922\n",
      "560: loss: 0.15323689267582571\n",
      "561: loss: 0.061658469261601574\n",
      "562: loss: 0.07586974004516377\n",
      "563: loss: 0.05997028459872429\n",
      "564: loss: 0.07935883696094001\n",
      "565: loss: 0.07407443354895803\n",
      "566: loss: 0.03640257514780387\n",
      "567: loss: 0.05584048345917836\n",
      "568: loss: 0.051184867149762184\n",
      "569: loss: 0.09763398902335514\n",
      "570: loss: 0.06058518035570159\n",
      "571: loss: 0.06023922164846832\n",
      "572: loss: 0.037225024338113144\n",
      "573: loss: 0.05028868252702523\n",
      "574: loss: 0.031056894607900173\n",
      "575: loss: 0.03664564907861252\n",
      "576: loss: 0.10234652772654348\n",
      "577: loss: 0.038780033394383885\n",
      "578: loss: 0.05366585514275358\n",
      "579: loss: 0.07562779103560995\n",
      "580: loss: 0.058116350937780226\n",
      "581: loss: 0.10275123361498117\n",
      "582: loss: 0.10477681365531075\n",
      "583: loss: 0.099142931275613\n",
      "584: loss: 0.1320693384786864\n",
      "585: loss: 0.047776055536814965\n",
      "586: loss: 0.11545197010612658\n",
      "587: loss: 0.0715546739908556\n",
      "588: loss: 0.07989731136088571\n",
      "589: loss: 0.04294410074362531\n",
      "590: loss: 0.08818896491235745\n",
      "591: loss: 0.049593121286307\n",
      "592: loss: 0.05463043522710602\n",
      "593: loss: 0.021793253574287515\n",
      "593: saving model with minimum loss to results\n",
      "594: loss: 0.06225787181756459\n",
      "595: loss: 0.06154527561739087\n",
      "596: loss: 0.0657369582525765\n",
      "597: loss: 0.04578405505162664\n",
      "598: loss: 0.07602805324131623\n",
      "599: loss: 0.08584288631633778\n",
      "600: loss: 0.0342003883383768\n",
      "601: loss: 0.06668737153813709\n",
      "602: loss: 0.05195392152139297\n",
      "603: loss: 0.025718886676865317\n",
      "604: loss: 0.04771801088160526\n",
      "605: loss: 0.08436665269255174\n",
      "606: loss: 0.03834281948608501\n",
      "607: loss: 0.10077500428693989\n",
      "608: loss: 0.06122723466251045\n",
      "609: loss: 0.050430194734265875\n",
      "610: loss: 0.05243518253943572\n",
      "611: loss: 0.0639963432283063\n",
      "612: loss: 0.030051843250324357\n",
      "613: loss: 0.031678324553164806\n",
      "614: loss: 0.10700253534984465\n",
      "615: loss: 0.04175843376045426\n",
      "616: loss: 0.03177251659508329\n",
      "617: loss: 0.06210945232305676\n",
      "618: loss: 0.04888580618232177\n",
      "619: loss: 0.04530726592444504\n",
      "620: loss: 0.04160894767846912\n",
      "621: loss: 0.033518759068101645\n",
      "622: loss: 0.05129992916287544\n",
      "623: loss: 0.05298536807822529\n",
      "624: loss: 0.061488375494567066\n",
      "625: loss: 0.09435758631237452\n",
      "626: loss: 0.059969118175407246\n",
      "627: loss: 0.06151022591317693\n",
      "628: loss: 0.038291142302720495\n",
      "629: loss: 0.01563344298241039\n",
      "629: saving model with minimum loss to results\n",
      "630: loss: 0.07242972751070435\n",
      "631: loss: 0.10883703470365923\n",
      "632: loss: 0.035070407124294434\n",
      "633: loss: 0.057685294566908844\n",
      "634: loss: 0.06698966732559104\n",
      "635: loss: 0.04817965421534609\n",
      "636: loss: 0.07694300221434483\n",
      "637: loss: 0.038536229779613976\n",
      "638: loss: 0.05450477843017627\n",
      "639: loss: 0.03711878242271875\n",
      "640: loss: 0.027025518473237753\n",
      "641: loss: 0.06527143778900306\n",
      "642: loss: 0.11936281863503002\n",
      "643: loss: 0.0718578081208155\n",
      "644: loss: 0.07165232468954248\n",
      "645: loss: 0.08603409375064074\n",
      "646: loss: 0.07100176415406168\n",
      "647: loss: 0.03898179354534173\n",
      "648: loss: 0.050425807479768985\n",
      "649: loss: 0.06094093224965034\n",
      "650: loss: 0.0391004179934195\n",
      "651: loss: 0.10455266611340146\n",
      "652: loss: 0.07467332614275315\n",
      "653: loss: 0.04389582764997613\n",
      "654: loss: 0.07049073566061756\n",
      "655: loss: 0.07633725782701123\n",
      "656: loss: 0.07343540421667663\n",
      "657: loss: 0.024589444510638717\n",
      "658: loss: 0.043780269380173806\n",
      "659: loss: 0.04506480468747517\n",
      "660: loss: 0.09507796609735428\n",
      "661: loss: 0.0429821942379931\n",
      "662: loss: 0.10286132842884398\n",
      "663: loss: 0.03351066843849064\n",
      "664: loss: 0.05257573397830129\n",
      "665: loss: 0.045851305980856225\n",
      "666: loss: 0.04326826952941096\n",
      "667: loss: 0.06231461514592713\n",
      "668: loss: 0.04183845690567978\n",
      "669: loss: 0.04288175497883155\n",
      "670: loss: 0.04832811110342542\n",
      "671: loss: 0.06239328005661567\n",
      "672: loss: 0.07792836246274722\n",
      "673: loss: 0.08345594488976835\n",
      "674: loss: 0.04280738582019695\n",
      "675: loss: 0.038312019663862884\n",
      "676: loss: 0.0336485878797248\n",
      "677: loss: 0.07954334441789494\n",
      "678: loss: 0.04161192065415284\n",
      "679: loss: 0.038356736224765584\n",
      "680: loss: 0.048921254929155104\n",
      "681: loss: 0.05323101750885448\n",
      "682: loss: 0.026803035618892565\n",
      "683: loss: 0.045164332957938313\n",
      "684: loss: 0.05054192318736265\n",
      "685: loss: 0.06309965035567681\n",
      "686: loss: 0.03717667322295408\n",
      "687: loss: 0.10394228191580623\n",
      "688: loss: 0.13530072154632458\n",
      "689: loss: 0.08945747157364774\n",
      "690: loss: 0.10881127213360742\n",
      "691: loss: 0.035662674044336505\n",
      "692: loss: 0.0397950674329574\n",
      "693: loss: 0.04473729025509479\n",
      "694: loss: 0.040966854949753426\n",
      "695: loss: 0.07692103165512283\n",
      "696: loss: 0.0888820358668454\n",
      "697: loss: 0.0527657024989215\n",
      "698: loss: 0.031863458585576154\n",
      "699: loss: 0.02166718143659333\n",
      "700: loss: 0.044531492361177996\n",
      "701: loss: 0.031235260462077957\n",
      "702: loss: 0.02505464041799617\n",
      "703: loss: 0.06661852211012348\n",
      "704: loss: 0.03582211232666547\n",
      "705: loss: 0.07862421177560465\n",
      "706: loss: 0.037239239376503974\n",
      "707: loss: 0.08348185935756192\n",
      "708: loss: 0.0706834178417921\n",
      "709: loss: 0.03794504967906202\n",
      "710: loss: 0.050673480436671525\n",
      "711: loss: 0.02416124245307098\n",
      "712: loss: 0.02360836765728891\n",
      "713: loss: 0.06006826594239101\n",
      "714: loss: 0.045617169962497435\n",
      "715: loss: 0.07494353009193824\n",
      "716: loss: 0.04090130922213575\n",
      "717: loss: 0.042239666935832546\n",
      "718: loss: 0.10956782114226371\n",
      "719: loss: 0.05177449845359661\n",
      "720: loss: 0.03565980842298207\n",
      "721: loss: 0.03396470621616269\n",
      "722: loss: 0.05148740317478465\n",
      "723: loss: 0.030588271853048354\n",
      "724: loss: 0.03495280995654563\n",
      "725: loss: 0.048464526553289033\n",
      "726: loss: 0.0415240217698738\n",
      "727: loss: 0.055961616608935096\n",
      "728: loss: 0.0774516399736361\n",
      "729: loss: 0.08670739994461958\n",
      "730: loss: 0.11104248954507057\n",
      "731: loss: 0.05575899611964512\n",
      "732: loss: 0.05699925640753159\n",
      "733: loss: 0.046845922654028975\n",
      "734: loss: 0.04364128309922914\n",
      "735: loss: 0.05247663862489087\n",
      "736: loss: 0.056860660047580794\n",
      "737: loss: 0.07808437887191153\n",
      "738: loss: 0.047839795413892716\n",
      "739: loss: 0.0451239785955598\n",
      "740: loss: 0.07531489285853847\n",
      "741: loss: 0.12284178156308674\n",
      "742: loss: 0.08942289759094516\n",
      "743: loss: 0.04309557615003238\n",
      "744: loss: 0.027418437084027875\n",
      "745: loss: 0.03604530298616737\n",
      "746: loss: 0.02257020395093908\n",
      "747: loss: 0.08242607212499328\n",
      "748: loss: 0.07579737901687622\n",
      "749: loss: 0.05243701834115199\n",
      "750: loss: 0.05322557402541861\n",
      "751: loss: 0.04071237282187213\n",
      "752: loss: 0.044800554273630645\n",
      "753: loss: 0.025626027517622184\n",
      "754: loss: 0.04565186330000869\n",
      "755: loss: 0.05988964684850848\n",
      "756: loss: 0.008156462160210745\n",
      "756: saving model with minimum loss to results\n",
      "757: loss: 0.0405471882938097\n",
      "758: loss: 0.020203791277405493\n",
      "759: loss: 0.06862881324680833\n",
      "760: loss: 0.02903023874387145\n",
      "761: loss: 0.029612255088674527\n",
      "762: loss: 0.11512990635431684\n",
      "763: loss: 0.0236972119382699\n",
      "764: loss: 0.09063433987709382\n",
      "765: loss: 0.0505918437265791\n",
      "766: loss: 0.041395161844169095\n",
      "767: loss: 0.0575085268355906\n",
      "768: loss: 0.05517716579682504\n",
      "769: loss: 0.04311872417262445\n",
      "770: loss: 0.07445277137837061\n",
      "771: loss: 0.04033471201546491\n",
      "772: loss: 0.04857835487291596\n",
      "773: loss: 0.02763567537961838\n",
      "774: loss: 0.03497475555195706\n",
      "775: loss: 0.03922385402256623\n",
      "776: loss: 0.04999224918234783\n",
      "777: loss: 0.03639935296572124\n",
      "778: loss: 0.06472817536753912\n",
      "779: loss: 0.04893647474818863\n",
      "780: loss: 0.05937928306714942\n",
      "781: loss: 0.04832281311973929\n",
      "782: loss: 0.024145951533379652\n",
      "783: loss: 0.09653328209727383\n",
      "784: loss: 0.02990266096700604\n",
      "785: loss: 0.029839983889056988\n",
      "786: loss: 0.03388197007977093\n",
      "787: loss: 0.08312705677235498\n",
      "788: loss: 0.05711974841930593\n",
      "789: loss: 0.09869592737716934\n",
      "790: loss: 0.029837903401736792\n",
      "791: loss: 0.06656040784825261\n",
      "792: loss: 0.03980983597769712\n",
      "793: loss: 0.030707702234697834\n",
      "794: loss: 0.0376134418863027\n",
      "795: loss: 0.05417584447422996\n",
      "796: loss: 0.07843110863177571\n",
      "797: loss: 0.047084764461033046\n",
      "798: loss: 0.07724606946188334\n",
      "799: loss: 0.05353033898669916\n",
      "800: loss: 0.059078144622617394\n",
      "801: loss: 0.032948014602879994\n",
      "802: loss: 0.04696207883534954\n",
      "803: loss: 0.029277270207482314\n",
      "804: loss: 0.057418848688636594\n",
      "805: loss: 0.034774211565187826\n",
      "806: loss: 0.06134719895635499\n",
      "807: loss: 0.02991146285785362\n",
      "808: loss: 0.06301316263852642\n",
      "809: loss: 0.07876832340843977\n",
      "810: loss: 0.019354168442077935\n",
      "811: loss: 0.05987752420818045\n",
      "812: loss: 0.07616866115616479\n",
      "813: loss: 0.0348426520358771\n",
      "814: loss: 0.05189772008452565\n",
      "815: loss: 0.05951459003457179\n",
      "816: loss: 0.0577275674440898\n",
      "817: loss: 0.025495788800374914\n",
      "818: loss: 0.07005791707585254\n",
      "819: loss: 0.028344740911658547\n",
      "820: loss: 0.028378931693926763\n",
      "821: loss: 0.06566114230857541\n",
      "822: loss: 0.02060322202548074\n",
      "823: loss: 0.04315632020977016\n",
      "824: loss: 0.06846512859920038\n",
      "825: loss: 0.037661802528115615\n",
      "826: loss: 0.05078052084718365\n",
      "827: loss: 0.08586500472544382\n",
      "828: loss: 0.013610825369444987\n",
      "829: loss: 0.02594383531929149\n",
      "830: loss: 0.04702488324255683\n",
      "831: loss: 0.049669247678442247\n",
      "832: loss: 0.034644595754798495\n",
      "833: loss: 0.06745095631292013\n",
      "834: loss: 0.05415780712307121\n",
      "835: loss: 0.0679067507056364\n",
      "836: loss: 0.0351305459626019\n",
      "837: loss: 0.03911393465629469\n",
      "838: loss: 0.04167608621840676\n",
      "839: loss: 0.046932026123007134\n",
      "840: loss: 0.0649457609397359\n",
      "841: loss: 0.03642945018267103\n",
      "842: loss: 0.0467759675035874\n",
      "843: loss: 0.03988706006202847\n",
      "844: loss: 0.07220245745944945\n",
      "845: loss: 0.02775334169079239\n",
      "846: loss: 0.052531473707252495\n",
      "847: loss: 0.026892214224668958\n",
      "848: loss: 0.02533708212043469\n",
      "849: loss: 0.04067961545661092\n",
      "850: loss: 0.07011155521225493\n",
      "851: loss: 0.03941344366952157\n",
      "852: loss: 0.04693183515216029\n",
      "853: loss: 0.044482451456133276\n",
      "854: loss: 0.01407587978368004\n",
      "855: loss: 0.032933675800450146\n",
      "856: loss: 0.06374263553880155\n",
      "857: loss: 0.023735183596727442\n",
      "858: loss: 0.019108893582597375\n",
      "859: loss: 0.05706039500546466\n",
      "860: loss: 0.05755766356984775\n",
      "861: loss: 0.026764614379014045\n",
      "862: loss: 0.05448601585036765\n",
      "863: loss: 0.054886663288925774\n",
      "864: loss: 0.047421902017958935\n",
      "865: loss: 0.049157988280057914\n",
      "866: loss: 0.04898611046761896\n",
      "867: loss: 0.04988188197603449\n",
      "868: loss: 0.018027588472856827\n",
      "869: loss: 0.041032721210892\n",
      "870: loss: 0.06396369319797184\n",
      "871: loss: 0.03052996013623973\n",
      "872: loss: 0.06580790202133356\n",
      "873: loss: 0.10572427419053079\n",
      "874: loss: 0.03831607380804295\n",
      "875: loss: 0.0659713726490736\n",
      "876: loss: 0.02930763134888063\n",
      "877: loss: 0.04636475612642243\n",
      "878: loss: 0.032841640621578946\n",
      "879: loss: 0.05594996803847607\n",
      "880: loss: 0.03282127688483646\n",
      "881: loss: 0.06391912931576371\n",
      "882: loss: 0.0387653834283507\n",
      "883: loss: 0.03985015557069952\n",
      "884: loss: 0.03530632165105392\n",
      "885: loss: 0.025148018496111032\n",
      "886: loss: 0.04748290500720032\n",
      "887: loss: 0.04131105864265314\n",
      "888: loss: 0.03888243606464433\n",
      "889: loss: 0.023890774721318547\n",
      "890: loss: 0.027283863086874284\n",
      "891: loss: 0.029953445783273004\n",
      "892: loss: 0.06911278135764101\n",
      "893: loss: 0.056190073208805785\n",
      "894: loss: 0.08155459138409543\n",
      "895: loss: 0.016634568969796724\n",
      "896: loss: 0.048655989366428315\n",
      "897: loss: 0.030596477561630305\n",
      "898: loss: 0.06922298025650282\n",
      "899: loss: 0.05388831968108814\n",
      "900: loss: 0.027334818514646034\n",
      "901: loss: 0.03948290112384711\n",
      "902: loss: 0.046390209036568805\n",
      "903: loss: 0.05153969969251194\n",
      "904: loss: 0.037096937361639\n",
      "905: loss: 0.07387686312237443\n",
      "906: loss: 0.026936839383173112\n",
      "907: loss: 0.03747611954653014\n",
      "908: loss: 0.036060346552403644\n",
      "909: loss: 0.031651617830599804\n",
      "910: loss: 0.03573308771107501\n",
      "911: loss: 0.015284595962536209\n",
      "912: loss: 0.052659068101396166\n",
      "913: loss: 0.0276274635301282\n",
      "914: loss: 0.0697184336410525\n",
      "915: loss: 0.02230130194220692\n",
      "916: loss: 0.07062302242654064\n",
      "917: loss: 0.0543365745106712\n",
      "918: loss: 0.028653745973618538\n",
      "919: loss: 0.05505363153739987\n",
      "920: loss: 0.03868428290782807\n",
      "921: loss: 0.03715684715037545\n",
      "922: loss: 0.05469018947527123\n",
      "923: loss: 0.06253409816417843\n",
      "924: loss: 0.023502483056896988\n",
      "925: loss: 0.023483903331604477\n",
      "926: loss: 0.021778305138771735\n",
      "927: loss: 0.016352258416494198\n",
      "928: loss: 0.024890923445733886\n",
      "929: loss: 0.032487084068028096\n",
      "930: loss: 0.09382340152903149\n",
      "931: loss: 0.05834636780976628\n",
      "932: loss: 0.04415332637290703\n",
      "933: loss: 0.05853663808617662\n",
      "934: loss: 0.09887609051656909\n",
      "935: loss: 0.04310438570488865\n",
      "936: loss: 0.017197335565773148\n",
      "937: loss: 0.019832947773466007\n",
      "938: loss: 0.047903270092016705\n",
      "939: loss: 0.054659835904506814\n",
      "940: loss: 0.023020790618223447\n",
      "941: loss: 0.023382174239183467\n",
      "942: loss: 0.07912470070489994\n",
      "943: loss: 0.029466622043401003\n",
      "944: loss: 0.05607786258527388\n",
      "945: loss: 0.0746945536302519\n",
      "946: loss: 0.05531664544226562\n",
      "947: loss: 0.0638545174151659\n",
      "948: loss: 0.038872751751720586\n",
      "949: loss: 0.03415458557568248\n",
      "950: loss: 0.03510403772816062\n",
      "951: loss: 0.01851305589661933\n",
      "952: loss: 0.02360479361959733\n",
      "953: loss: 0.06873083291854225\n",
      "954: loss: 0.026086208061315123\n",
      "955: loss: 0.02210384008746284\n",
      "956: loss: 0.03340857229583586\n",
      "957: loss: 0.06301890480487296\n",
      "958: loss: 0.04392206486469755\n",
      "959: loss: 0.031144129219076905\n",
      "960: loss: 0.04333504646395643\n",
      "961: loss: 0.030073234433075413\n",
      "962: loss: 0.09157037696180244\n",
      "963: loss: 0.03973611106630415\n",
      "964: loss: 0.024493619722003736\n",
      "965: loss: 0.04021238496837517\n",
      "966: loss: 0.04793083161348476\n",
      "967: loss: 0.060882200583970786\n",
      "968: loss: 0.049977540775823094\n",
      "969: loss: 0.07741677407708873\n",
      "970: loss: 0.015875105532662324\n",
      "971: loss: 0.028639523632591594\n",
      "972: loss: 0.0732757260047947\n",
      "973: loss: 0.01718978314117218\n",
      "974: loss: 0.028082726047917585\n",
      "975: loss: 0.08247535363382973\n",
      "976: loss: 0.02333326486404985\n",
      "977: loss: 0.06193585234965819\n",
      "978: loss: 0.032956621483511604\n",
      "979: loss: 0.01684897835366428\n",
      "980: loss: 0.04666448779043245\n",
      "981: loss: 0.025398908590432253\n",
      "982: loss: 0.02924092098449668\n",
      "983: loss: 0.04412670566913827\n",
      "984: loss: 0.024654552573338148\n",
      "985: loss: 0.023693430186540354\n",
      "986: loss: 0.017075918498449028\n",
      "987: loss: 0.052766410051845014\n",
      "988: loss: 0.029168631338203948\n",
      "989: loss: 0.03546922941071291\n",
      "990: loss: 0.03493293316569179\n",
      "991: loss: 0.03658654439883927\n",
      "992: loss: 0.027008801155413188\n",
      "993: loss: 0.039384922200648965\n",
      "994: loss: 0.06143618727219291\n",
      "995: loss: 0.014599031484370547\n",
      "996: loss: 0.05687265765542785\n",
      "997: loss: 0.03865841147489846\n",
      "998: loss: 0.04286931546327347\n",
      "999: loss: 0.044332215751637705\n",
      "1000: loss: 0.043352120239433134\n",
      "1000: saving model to results\n",
      "1001: loss: 0.034669627745946244\n",
      "1002: loss: 0.0354574330849573\n",
      "1003: loss: 0.02126120160877084\n",
      "1004: loss: 0.021819834756267174\n",
      "1005: loss: 0.03715401142835617\n",
      "1006: loss: 0.03938089206349105\n",
      "1007: loss: 0.03842612530570477\n",
      "1008: loss: 0.029935492309353617\n",
      "1009: loss: 0.02090148604474962\n",
      "1010: loss: 0.0193765500249962\n",
      "1011: loss: 0.07335373878595418\n",
      "1012: loss: 0.06038892298238353\n",
      "1013: loss: 0.05115150970717271\n",
      "1014: loss: 0.09091562527464703\n",
      "1015: loss: 0.033587555984922794\n",
      "1016: loss: 0.02161574367589007\n",
      "1017: loss: 0.06645981938345358\n",
      "1018: loss: 0.026966413444218535\n",
      "1019: loss: 0.01882874783283721\n",
      "1020: loss: 0.031900040611314275\n",
      "1021: loss: 0.035171697123587364\n",
      "1022: loss: 0.05357264990258652\n",
      "1023: loss: 0.024455235902375236\n",
      "1024: loss: 0.021039634671372674\n",
      "1025: loss: 0.03117225139673489\n",
      "1026: loss: 0.02699950186070055\n",
      "1027: loss: 0.0313899964094162\n",
      "1028: loss: 0.05899280869440796\n",
      "1029: loss: 0.02066490408712222\n",
      "1030: loss: 0.04445890938707938\n",
      "1031: loss: 0.03840032631220917\n",
      "1032: loss: 0.07529937545768917\n",
      "1033: loss: 0.05183808330912142\n",
      "1034: loss: 0.022530923112450786\n",
      "1035: loss: 0.023742550622652438\n",
      "1036: loss: 0.03174872376257554\n",
      "1037: loss: 0.025202584103681147\n",
      "1038: loss: 0.015577425307128577\n",
      "1039: loss: 0.040844588103936985\n",
      "1040: loss: 0.016969766623030107\n",
      "1041: loss: 0.050039283853645124\n",
      "1042: loss: 0.023307450985763957\n",
      "1043: loss: 0.03745676903054118\n",
      "1044: loss: 0.03302413922695754\n",
      "1045: loss: 0.017160447586017348\n",
      "1046: loss: 0.018111699442670215\n",
      "1047: loss: 0.014931435754988343\n",
      "1048: loss: 0.025856073819644127\n",
      "1049: loss: 0.06150503160461085\n",
      "1050: loss: 0.013740646025932314\n",
      "1051: loss: 0.022390142684647188\n",
      "1052: loss: 0.0612369681087633\n",
      "1053: loss: 0.030016662455939994\n",
      "1054: loss: 0.05111600347724258\n",
      "1055: loss: 0.03766289970371872\n",
      "1056: loss: 0.053588464138253286\n",
      "1057: loss: 0.03369765150515984\n",
      "1058: loss: 0.02528945813052511\n",
      "1059: loss: 0.026412753485298406\n",
      "1060: loss: 0.027679080486753563\n",
      "1061: loss: 0.027085210157868765\n",
      "1062: loss: 0.04195135359865768\n",
      "1063: loss: 0.03574315628308493\n",
      "1064: loss: 0.013123628777975682\n",
      "1065: loss: 0.033789765296717185\n",
      "1066: loss: 0.08764809934655204\n",
      "1067: loss: 0.04978812337503769\n",
      "1068: loss: 0.04088149479260514\n",
      "1069: loss: 0.018507458644914248\n",
      "1070: loss: 0.05448207681183703\n",
      "1071: loss: 0.01590043427616668\n",
      "1072: loss: 0.030366587326473866\n",
      "1073: loss: 0.025714883478940468\n",
      "1074: loss: 0.02232524610008113\n",
      "1075: loss: 0.010869266572020326\n",
      "1076: loss: 0.06579577476562311\n",
      "1077: loss: 0.03264222203870304\n",
      "1078: loss: 0.026678347290726375\n",
      "1079: loss: 0.0225163533371718\n",
      "1080: loss: 0.012676021540149426\n",
      "1081: loss: 0.033485249650993865\n",
      "1082: loss: 0.025470278448968504\n",
      "1083: loss: 0.02841463698617493\n",
      "1084: loss: 0.03290433190219725\n",
      "1085: loss: 0.02194026931344221\n",
      "1086: loss: 0.04675851307304887\n",
      "1087: loss: 0.04855924542061985\n",
      "1088: loss: 0.02235110028414056\n",
      "1089: loss: 0.0271866392577067\n",
      "1090: loss: 0.03844070434570312\n",
      "1091: loss: 0.08119522878647936\n",
      "1092: loss: 0.016705405740746453\n",
      "1093: loss: 0.01729401317425072\n",
      "1094: loss: 0.019396034845461447\n",
      "1095: loss: 0.02249869978792655\n",
      "1096: loss: 0.022952386245985206\n",
      "1097: loss: 0.0552435014663691\n",
      "1098: loss: 0.051723114221355594\n",
      "1099: loss: 0.029029743454884738\n",
      "1100: loss: 0.033271377556957304\n",
      "1101: loss: 0.02114880786151237\n",
      "1102: loss: 0.041001025044048824\n",
      "1103: loss: 0.03385856592406829\n",
      "1104: loss: 0.05798863725794944\n",
      "1105: loss: 0.02325207697549558\n",
      "1106: loss: 0.036963358734889575\n",
      "1107: loss: 0.07269503727244837\n",
      "1108: loss: 0.0131800922366286\n",
      "1109: loss: 0.03286082192789763\n",
      "1110: loss: 0.023089761418911316\n",
      "1111: loss: 0.037893790615877755\n",
      "1112: loss: 0.01393550922027013\n",
      "1113: loss: 0.041647987362618245\n",
      "1114: loss: 0.05350769194774329\n",
      "1115: loss: 0.015044618747197092\n",
      "1116: loss: 0.043993359237598874\n",
      "1117: loss: 0.06067274774735172\n",
      "1118: loss: 0.030869210274734847\n",
      "1119: loss: 0.01911137571247916\n",
      "1120: loss: 0.03009943648551901\n",
      "1121: loss: 0.025947977594720822\n",
      "1122: loss: 0.007761456266356012\n",
      "1122: saving model with minimum loss to results\n",
      "1123: loss: 0.029839903513978545\n",
      "1124: loss: 0.023335313586964432\n",
      "1125: loss: 0.039454516275630645\n",
      "1126: loss: 0.009858410727853574\n",
      "1127: loss: 0.028032096937143553\n",
      "1128: loss: 0.01013709549442865\n",
      "1129: loss: 0.02201022083560626\n",
      "1130: loss: 0.013800319429719819\n",
      "1131: loss: 0.024111888526628416\n",
      "1132: loss: 0.048862708713083215\n",
      "1133: loss: 0.030095825107612956\n",
      "1134: loss: 0.04322012638052305\n",
      "1135: loss: 0.055959342552038535\n",
      "1136: loss: 0.037558054861923054\n",
      "1137: loss: 0.05111671389507439\n",
      "1138: loss: 0.024654303884744877\n",
      "1139: loss: 0.02136246739246417\n",
      "1140: loss: 0.02607002155855298\n",
      "1141: loss: 0.040916960952017696\n",
      "1142: loss: 0.02449696542074283\n",
      "1143: loss: 0.03601946464429299\n",
      "1144: loss: 0.031390814964349076\n",
      "1145: loss: 0.04698405715074235\n",
      "1146: loss: 0.020514170745930944\n",
      "1147: loss: 0.049930357756238664\n",
      "1148: loss: 0.021345555122631286\n",
      "1149: loss: 0.053966434963513166\n",
      "1150: loss: 0.03220147437726458\n",
      "1151: loss: 0.03320060330831135\n",
      "1152: loss: 0.03138304509532948\n",
      "1153: loss: 0.03224127607730528\n",
      "1154: loss: 0.01269046434511741\n",
      "1155: loss: 0.028165380237624053\n",
      "1156: loss: 0.05530434288084507\n",
      "1157: loss: 0.02235920591435085\n",
      "1158: loss: 0.01257374599420776\n",
      "1159: loss: 0.03249750177686413\n",
      "1160: loss: 0.042293213014393885\n",
      "1161: loss: 0.08738454157234327\n",
      "1162: loss: 0.02414037928974722\n",
      "1163: loss: 0.01880577361832062\n",
      "1164: loss: 0.029049501618525635\n",
      "1165: loss: 0.03179368036217056\n",
      "1166: loss: 0.0437159522358949\n",
      "1167: loss: 0.018272145292333637\n",
      "1168: loss: 0.04959487451318031\n",
      "1169: loss: 0.01577864993790475\n",
      "1170: loss: 0.04178049190280338\n",
      "1171: loss: 0.0337412531953305\n",
      "1172: loss: 0.030152793372205142\n",
      "1173: loss: 0.044740130377855764\n",
      "1174: loss: 0.022199614749600492\n",
      "1175: loss: 0.04938227533906077\n",
      "1176: loss: 0.025354583813168574\n",
      "1177: loss: 0.03297720954287797\n",
      "1178: loss: 0.026504025613879396\n",
      "1179: loss: 0.030109356054405605\n",
      "1180: loss: 0.03679798568676536\n",
      "1181: loss: 0.013355355554570757\n",
      "1182: loss: 0.0257923581424014\n",
      "1183: loss: 0.05252339648238073\n",
      "1184: loss: 0.022188149399880785\n",
      "1185: loss: 0.04241572766477475\n",
      "1186: loss: 0.015051877756680673\n",
      "1187: loss: 0.05169848752363275\n",
      "1188: loss: 0.04480468965872812\n",
      "1189: loss: 0.03306402066846204\n",
      "1190: loss: 0.025318639003671702\n",
      "1191: loss: 0.025871161604300145\n",
      "1192: loss: 0.024320600257245435\n",
      "1193: loss: 0.035756418898624055\n",
      "1194: loss: 0.017645275779917334\n",
      "1195: loss: 0.03238786780275405\n",
      "1196: loss: 0.039349751042512544\n",
      "1197: loss: 0.024913530893779044\n",
      "1198: loss: 0.03727267301292159\n",
      "1199: loss: 0.03360290419853603\n",
      "1200: loss: 0.02936384648395081\n",
      "1201: loss: 0.026479174848645926\n",
      "1202: loss: 0.027459955075755715\n",
      "1203: loss: 0.06640249528330362\n",
      "1204: loss: 0.06892477449340123\n",
      "1205: loss: 0.013047322048805654\n",
      "1206: loss: 0.026786442380398515\n",
      "1207: loss: 0.024204032961279157\n",
      "1208: loss: 0.024125274542408683\n",
      "1209: loss: 0.023855281057573546\n",
      "1210: loss: 0.060139927120568856\n",
      "1211: loss: 0.022500365128507834\n",
      "1212: loss: 0.03164061015316596\n",
      "1213: loss: 0.05426456736555943\n",
      "1214: loss: 0.07231552091737589\n",
      "1215: loss: 0.06154657434672117\n",
      "1216: loss: 0.03631607906815285\n",
      "1217: loss: 0.04350511624943464\n",
      "1218: loss: 0.01856578020184922\n",
      "1219: loss: 0.05077242612605914\n",
      "1220: loss: 0.012826644970724981\n",
      "1221: loss: 0.020516398129984733\n",
      "1222: loss: 0.05914267575523506\n",
      "1223: loss: 0.04338650855546196\n",
      "1224: loss: 0.02253541102497062\n",
      "1225: loss: 0.035106679133605205\n",
      "1226: loss: 0.028274580254219472\n",
      "1227: loss: 0.027828913919317223\n",
      "1228: loss: 0.02739663790756216\n",
      "1229: loss: 0.014636888857542846\n",
      "1230: loss: 0.027048609306802977\n",
      "1231: loss: 0.015856989023935363\n",
      "1232: loss: 0.02648408425739035\n",
      "1233: loss: 0.017866867283980053\n",
      "1234: loss: 0.02709293617711713\n",
      "1235: loss: 0.03147792247667287\n",
      "1236: loss: 0.028471159341279417\n",
      "1237: loss: 0.02650251601880882\n",
      "1238: loss: 0.02027445157485393\n",
      "1239: loss: 0.047803066064564824\n",
      "1240: loss: 0.023251247320634622\n",
      "1241: loss: 0.025221132033038884\n",
      "1242: loss: 0.044482656279190756\n",
      "1243: loss: 0.018144609411441102\n",
      "1244: loss: 0.04432984117496138\n",
      "1245: loss: 0.02280066969494025\n",
      "1246: loss: 0.010233437409624457\n",
      "1247: loss: 0.024431870008508362\n",
      "1248: loss: 0.018083997303619977\n",
      "1249: loss: 0.09475430251526025\n",
      "1250: loss: 0.024047047637092568\n",
      "1251: loss: 0.04361342466048276\n",
      "1252: loss: 0.05939561730095497\n",
      "1253: loss: 0.015331486608677853\n",
      "1254: loss: 0.01593499188311398\n",
      "1255: loss: 0.04286057187709958\n",
      "1256: loss: 0.026786043103735818\n",
      "1257: loss: 0.06037221598671749\n",
      "1258: loss: 0.019992148542466264\n",
      "1259: loss: 0.017365073474744957\n",
      "1260: loss: 0.025993650934348505\n",
      "1261: loss: 0.014762637981524069\n",
      "1262: loss: 0.03593395920082306\n",
      "1263: loss: 0.03232694406566831\n",
      "1264: loss: 0.031804409013905875\n",
      "1265: loss: 0.04701188543306974\n",
      "1266: loss: 0.014859092130791392\n",
      "1267: loss: 0.03237634831748437\n",
      "1268: loss: 0.034660276297169425\n",
      "1269: loss: 0.025262048293370757\n",
      "1270: loss: 0.04603831993881613\n",
      "1271: loss: 0.03078374573184798\n",
      "1272: loss: 0.04609006131067872\n",
      "1273: loss: 0.05318512899490695\n",
      "1274: loss: 0.016441504538912948\n",
      "1275: loss: 0.0783199586852182\n",
      "1276: loss: 0.014922904102907825\n",
      "1277: loss: 0.04341811387954901\n",
      "1278: loss: 0.025418906634513405\n",
      "1279: loss: 0.05392021270624051\n",
      "1280: loss: 0.028961559699382636\n",
      "1281: loss: 0.016179312020540237\n",
      "1282: loss: 0.07782852028807004\n",
      "1283: loss: 0.014537459554655166\n",
      "1284: loss: 0.04532415565336123\n",
      "1285: loss: 0.049239231176519155\n",
      "1286: loss: 0.03532795536254223\n",
      "1287: loss: 0.04774186560340847\n",
      "1288: loss: 0.019377226262198143\n",
      "1289: loss: 0.052206825076912836\n",
      "1290: loss: 0.019637264689663425\n",
      "1291: loss: 0.0378130393801257\n",
      "1292: loss: 0.02534863922240523\n",
      "1293: loss: 0.03250048676272854\n",
      "1294: loss: 0.03970294589331995\n",
      "1295: loss: 0.029495234455680475\n",
      "1296: loss: 0.042189352214336395\n",
      "1297: loss: 0.019875578912130248\n",
      "1298: loss: 0.035099647395933666\n",
      "1299: loss: 0.02593866592117896\n",
      "1300: loss: 0.03102591185597703\n",
      "1301: loss: 0.06937162873752338\n",
      "1302: loss: 0.04759876485331915\n",
      "1303: loss: 0.04200900714689245\n",
      "1304: loss: 0.009308802720624954\n",
      "1305: loss: 0.0057305680044616265\n",
      "1305: saving model with minimum loss to results\n",
      "1306: loss: 0.010055782428632181\n",
      "1307: loss: 0.03765160474965038\n",
      "1308: loss: 0.02124296361580491\n",
      "1309: loss: 0.048324911195474364\n",
      "1310: loss: 0.016850804822752252\n",
      "1311: loss: 0.03075848030857742\n",
      "1312: loss: 0.020442016442151118\n",
      "1313: loss: 0.016345036410105724\n",
      "1314: loss: 0.012574848476409292\n",
      "1315: loss: 0.014467724822073555\n",
      "1316: loss: 0.030396024541308478\n",
      "1317: loss: 0.025164483454621706\n",
      "1318: loss: 0.046039801847655326\n",
      "1319: loss: 0.014618123988232886\n",
      "1320: loss: 0.03636456691310742\n",
      "1321: loss: 0.02469936738877247\n",
      "1322: loss: 0.005946319298042606\n",
      "1323: loss: 0.024209546675895883\n",
      "1324: loss: 0.032651196073857136\n",
      "1325: loss: 0.022446318141495187\n",
      "1326: loss: 0.017262043079729967\n",
      "1327: loss: 0.06827332448059073\n",
      "1328: loss: 0.019622187110750623\n",
      "1329: loss: 0.03661514806056705\n",
      "1330: loss: 0.021782637476765863\n",
      "1331: loss: 0.024312059627845883\n",
      "1332: loss: 0.005161428765859456\n",
      "1332: saving model with minimum loss to results\n",
      "1333: loss: 0.01951494474390832\n",
      "1334: loss: 0.052835564594715834\n",
      "1335: loss: 0.02242313713456194\n",
      "1336: loss: 0.02479831443027555\n",
      "1337: loss: 0.02714277050108649\n",
      "1338: loss: 0.04109452474707116\n",
      "1339: loss: 0.02656692425565173\n",
      "1340: loss: 0.02703079286341866\n",
      "1341: loss: 0.04283312777988612\n",
      "1342: loss: 0.010543888066119205\n",
      "1343: loss: 0.011931675563876826\n",
      "1344: loss: 0.031239048655455317\n",
      "1345: loss: 0.010790138030036664\n",
      "1346: loss: 0.0162333053497908\n",
      "1347: loss: 0.05370922852307558\n",
      "1348: loss: 0.038736569986213\n",
      "1349: loss: 0.04710009241534863\n",
      "1350: loss: 0.02467424869731379\n",
      "1351: loss: 0.037737287736187376\n",
      "1352: loss: 0.07383487346426894\n",
      "1353: loss: 0.013647928562325735\n",
      "1354: loss: 0.017504187882877883\n",
      "1355: loss: 0.05285684931247184\n",
      "1356: loss: 0.03949277984793297\n",
      "1357: loss: 0.04682748469834526\n",
      "1358: loss: 0.03273658958884577\n",
      "1359: loss: 0.052812533627729856\n",
      "1360: loss: 0.008484967246962091\n",
      "1361: loss: 0.03477871063660132\n",
      "1362: loss: 0.04301743372586013\n",
      "1363: loss: 0.023743116102802254\n",
      "1364: loss: 0.07130410387859834\n",
      "1365: loss: 0.009245510135466855\n",
      "1366: loss: 0.05579729713887597\n",
      "1367: loss: 0.038106820526688054\n",
      "1368: loss: 0.029539351584389806\n",
      "1369: loss: 0.04983963538688841\n",
      "1370: loss: 0.0737390274880454\n",
      "1371: loss: 0.013260009232908487\n",
      "1372: loss: 0.041119140349716574\n",
      "1373: loss: 0.04016678597933302\n",
      "1374: loss: 0.01861857098022786\n",
      "1375: loss: 0.011306728934869172\n",
      "1376: loss: 0.022736738746364907\n",
      "1377: loss: 0.03214484938265135\n",
      "1378: loss: 0.03877619359991513\n",
      "1379: loss: 0.04348906932864339\n",
      "1380: loss: 0.03692110488191248\n",
      "1381: loss: 0.049655380522987486\n",
      "1382: loss: 0.023341381902961682\n",
      "1383: loss: 0.02204787024917702\n",
      "1384: loss: 0.02039853603734324\n",
      "1385: loss: 0.028077587965526618\n",
      "1386: loss: 0.04049637199689945\n",
      "1387: loss: 0.04961074804305099\n",
      "1388: loss: 0.041411922633415074\n",
      "1389: loss: 0.05497002439612213\n",
      "1390: loss: 0.01865225739311427\n",
      "1391: loss: 0.03688571884777048\n",
      "1392: loss: 0.013545616180635989\n",
      "1393: loss: 0.040324503381270915\n",
      "1394: loss: 0.044061287927130856\n",
      "1395: loss: 0.03756625225651078\n",
      "1396: loss: 0.04118637403007597\n",
      "1397: loss: 0.02590161605985486\n",
      "1398: loss: 0.024834395948952686\n",
      "1399: loss: 0.03012382103285442\n",
      "1400: loss: 0.03384524814706917\n",
      "1401: loss: 0.010706069607598087\n",
      "1402: loss: 0.03307328361552209\n",
      "1403: loss: 0.03598589384152244\n",
      "1404: loss: 0.011021188615510862\n",
      "1405: loss: 0.022300306047933798\n",
      "1406: loss: 0.02001103656706012\n",
      "1407: loss: 0.03313114517368376\n",
      "1408: loss: 0.05035207403125241\n",
      "1409: loss: 0.01510088339758416\n",
      "1410: loss: 0.024797364739545927\n",
      "1411: loss: 0.0318716352339834\n",
      "1412: loss: 0.020855771377682682\n",
      "1413: loss: 0.07094583673339609\n",
      "1414: loss: 0.028244382119737562\n",
      "1415: loss: 0.03764287579300193\n",
      "1416: loss: 0.02755559538976134\n",
      "1417: loss: 0.024590442757471465\n",
      "1418: loss: 0.03575815304551118\n",
      "1419: loss: 0.03619195018351699\n",
      "1420: loss: 0.02210923672343294\n",
      "1421: loss: 0.020746278877292447\n",
      "1422: loss: 0.0199179480841849\n",
      "1423: loss: 0.05737197701819241\n",
      "1424: loss: 0.031737762711903386\n",
      "1425: loss: 0.030751882038506057\n",
      "1426: loss: 0.04016170336156696\n",
      "1427: loss: 0.01698992036593457\n",
      "1428: loss: 0.030871918172730755\n",
      "1429: loss: 0.05643966396261627\n",
      "1430: loss: 0.04977456259075552\n",
      "1431: loss: 0.052673870620007314\n",
      "1432: loss: 0.04123405465452379\n",
      "1433: loss: 0.04624654544750229\n",
      "1434: loss: 0.015937377950952698\n",
      "1435: loss: 0.024561317152498912\n",
      "1436: loss: 0.0037666609471974275\n",
      "1436: saving model with minimum loss to results\n",
      "1437: loss: 0.03097324573900551\n",
      "1438: loss: 0.05704769177827984\n",
      "1439: loss: 0.014117358815080177\n",
      "1440: loss: 0.01406445251389717\n",
      "1441: loss: 0.02018809740548022\n",
      "1442: loss: 0.031278505552715316\n",
      "1443: loss: 0.017475640369715016\n",
      "1444: loss: 0.013776253947677713\n",
      "1445: loss: 0.028971177370597918\n",
      "1446: loss: 0.02359440134993444\n",
      "1447: loss: 0.04481486495933495\n",
      "1448: loss: 0.03702028945554048\n",
      "1449: loss: 0.04239577602614494\n",
      "1450: loss: 0.015055572284230342\n",
      "1451: loss: 0.021452989807585254\n",
      "1452: loss: 0.04436203464865685\n",
      "1453: loss: 0.010220354328339454\n",
      "1454: loss: 0.008469920294980208\n",
      "1455: loss: 0.01627280858034889\n",
      "1456: loss: 0.04152218838377545\n",
      "1457: loss: 0.03682961204807119\n",
      "1458: loss: 0.018957014234426122\n",
      "1459: loss: 0.020085705541229498\n",
      "1460: loss: 0.022348510547696297\n",
      "1461: loss: 0.032821661082077\n",
      "1462: loss: 0.027643154899124056\n",
      "1463: loss: 0.016537629417143762\n",
      "1464: loss: 0.02818287268746644\n",
      "1465: loss: 0.02427610328110556\n",
      "1466: loss: 0.033048262392791614\n",
      "1467: loss: 0.052232143973621234\n",
      "1468: loss: 0.028955023779417385\n",
      "1469: loss: 0.03766997893884157\n",
      "1470: loss: 0.020447200979106128\n",
      "1471: loss: 0.03750015303861194\n",
      "1472: loss: 0.01094544962203751\n",
      "1473: loss: 0.05955231901801501\n",
      "1474: loss: 0.01619830095053961\n",
      "1475: loss: 0.01567781763151288\n",
      "1476: loss: 0.017604624646385972\n",
      "1477: loss: 0.022636502451253662\n",
      "1478: loss: 0.04354413870411614\n",
      "1479: loss: 0.040375600336119526\n",
      "1480: loss: 0.02354724185230831\n",
      "1481: loss: 0.038837754918252664\n",
      "1482: loss: 0.03517300315434113\n",
      "1483: loss: 0.0369150311841319\n",
      "1484: loss: 0.04831315200620641\n",
      "1485: loss: 0.015028304187580945\n",
      "1486: loss: 0.02529639819355604\n",
      "1487: loss: 0.018325870196955904\n",
      "1488: loss: 0.05013777205507117\n",
      "1489: loss: 0.02435734196721266\n",
      "1490: loss: 0.03283660439774394\n",
      "1491: loss: 0.017194442043546587\n",
      "1492: loss: 0.03613559086807072\n",
      "1493: loss: 0.04156628962583635\n",
      "1494: loss: 0.02690239231257389\n",
      "1495: loss: 0.007767579867504537\n",
      "1496: loss: 0.051525660092011094\n",
      "1497: loss: 0.028219514035299653\n",
      "1498: loss: 0.02139307469284783\n",
      "1499: loss: 0.016214443189634647\n",
      "1500: loss: 0.025180772441672165\n",
      "1501: loss: 0.04186978372551191\n",
      "1502: loss: 0.0231599360704422\n",
      "1503: loss: 0.016566697362577543\n",
      "1504: loss: 0.015064840059494598\n",
      "1505: loss: 0.025902993693307508\n",
      "1506: loss: 0.05089345345428834\n",
      "1507: loss: 0.022799674576769274\n",
      "1508: loss: 0.02722389720050463\n",
      "1509: loss: 0.011965058976784348\n",
      "1510: loss: 0.020139197042832773\n",
      "1511: loss: 0.04881589238842328\n",
      "1512: loss: 0.039980496127100196\n",
      "1513: loss: 0.02730043961976965\n",
      "1514: loss: 0.026341576769482348\n",
      "1515: loss: 0.03652860678266734\n",
      "1516: loss: 0.026456454167297732\n",
      "1517: loss: 0.017594356220797632\n",
      "1518: loss: 0.02039891527965665\n",
      "1519: loss: 0.023084563649414726\n",
      "1520: loss: 0.005351309383210416\n",
      "1521: loss: 0.04050256516347872\n",
      "1522: loss: 0.019152462909308575\n",
      "1523: loss: 0.03060917401065429\n",
      "1524: loss: 0.02566100796684623\n",
      "1525: loss: 0.045596069062109265\n",
      "1526: loss: 0.0228414423763752\n",
      "1527: loss: 0.0180472710247462\n",
      "1528: loss: 0.01930803289481749\n",
      "1529: loss: 0.027734208755039923\n",
      "1530: loss: 0.03729220211971551\n",
      "1531: loss: 0.006213447476814811\n",
      "1532: loss: 0.015209233368902156\n",
      "1533: loss: 0.017265127651626244\n",
      "1534: loss: 0.033706836921434544\n",
      "1535: loss: 0.02265987442418312\n",
      "1536: loss: 0.012180529864660151\n",
      "1537: loss: 0.02308520715450868\n",
      "1538: loss: 0.035620360041017804\n",
      "1539: loss: 0.03655119092824559\n",
      "1540: loss: 0.02520149461148928\n",
      "1541: loss: 0.019685581287679572\n",
      "1542: loss: 0.05393202757113612\n",
      "1543: loss: 0.017124554152057197\n",
      "1544: loss: 0.03572106586458782\n",
      "1545: loss: 0.03059606163878925\n",
      "1546: loss: 0.021025266401314486\n",
      "1547: loss: 0.01930001940733443\n",
      "1548: loss: 0.023598345733868577\n",
      "1549: loss: 0.010668760631233455\n",
      "1550: loss: 0.039316355161039\n",
      "1551: loss: 0.023353504521461826\n",
      "1552: loss: 0.02060024930203023\n",
      "1553: loss: 0.035965148728185646\n",
      "1554: loss: 0.030062210280448195\n",
      "1555: loss: 0.028185879855300296\n",
      "1556: loss: 0.0341078190637442\n",
      "1557: loss: 0.026845301423842706\n",
      "1558: loss: 0.02337135665584356\n",
      "1559: loss: 0.032270970734922834\n",
      "1560: loss: 0.04253456621275594\n",
      "1561: loss: 0.03635797114111483\n",
      "1562: loss: 0.017376318234407037\n",
      "1563: loss: 0.03522948826321226\n",
      "1564: loss: 0.028736265434417874\n",
      "1565: loss: 0.07640365430415841\n",
      "1566: loss: 0.03127738237283968\n",
      "1567: loss: 0.02452141612108486\n",
      "1568: loss: 0.04125880483964768\n",
      "1569: loss: 0.021451325582650796\n",
      "1570: loss: 0.015871560860735674\n",
      "1571: loss: 0.04831237587495706\n",
      "1572: loss: 0.01597700441683022\n",
      "1573: loss: 0.012490381467311332\n",
      "1574: loss: 0.017496679686397936\n",
      "1575: loss: 0.04574458951052899\n",
      "1576: loss: 0.047497646786117315\n",
      "1577: loss: 0.03297622478567064\n",
      "1578: loss: 0.01906407528137303\n",
      "1579: loss: 0.012350929726380857\n",
      "1580: loss: 0.02474035473035959\n",
      "1581: loss: 0.026848287088796493\n",
      "1582: loss: 0.016195250471355394\n",
      "1583: loss: 0.018151300959289074\n",
      "1584: loss: 0.026153235657451052\n",
      "1585: loss: 0.017209562967764214\n",
      "1586: loss: 0.03017427040807282\n",
      "1587: loss: 0.04534234356833621\n",
      "1588: loss: 0.011973985577545438\n",
      "1589: loss: 0.02146427798409907\n",
      "1590: loss: 0.06880102546225922\n",
      "1591: loss: 0.055942860043918095\n",
      "1592: loss: 0.02661064975351716\n",
      "1593: loss: 0.037636222152893126\n",
      "1594: loss: 0.028777285194640474\n",
      "1595: loss: 0.02228957356419414\n",
      "1596: loss: 0.02289304406440351\n",
      "1597: loss: 0.026028835137064252\n",
      "1598: loss: 0.027153278701007366\n",
      "1599: loss: 0.03162526180191587\n",
      "1600: loss: 0.019248143168321498\n",
      "1601: loss: 0.02399404158253067\n",
      "1602: loss: 0.01410037892734787\n",
      "1603: loss: 0.010530235304031521\n",
      "1604: loss: 0.022682505688862875\n",
      "1605: loss: 0.035698997737199534\n",
      "1606: loss: 0.014064893320513267\n",
      "1607: loss: 0.04426686900357406\n",
      "1608: loss: 0.038868646787401914\n",
      "1609: loss: 0.021987642666014533\n",
      "1610: loss: 0.01965588080929592\n",
      "1611: loss: 0.011194080412678886\n",
      "1612: loss: 0.025361592478778526\n",
      "1613: loss: 0.009720146947074682\n",
      "1614: loss: 0.03851682679184402\n",
      "1615: loss: 0.01969654758674248\n",
      "1616: loss: 0.028429955632115405\n",
      "1617: loss: 0.024979916039834887\n",
      "1618: loss: 0.012156956117754211\n",
      "1619: loss: 0.026123885778360997\n",
      "1620: loss: 0.06972705594913957\n",
      "1621: loss: 0.01448077355356266\n",
      "1622: loss: 0.016801780652410038\n",
      "1623: loss: 0.03512363935199877\n",
      "1624: loss: 0.014602504204958677\n",
      "1625: loss: 0.03607629622517076\n",
      "1626: loss: 0.010642182858039934\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmulan_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<@beartype(musiclm_pytorch.trainer.MuLaNTrainer.train) at 0x7f0d4046af80>:32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(__beartype_object_25065568, __beartype_get_violation, __beartype_conf, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/music_decomposition/musiclm-pytorch/musiclm_pytorch/trainer.py:363\u001b[0m, in \u001b[0;36mMuLaNTrainer.train\u001b[0;34m(self, log_fn)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_fn: Callable \u001b[38;5;241m=\u001b[39m noop):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_steps:\n\u001b[0;32m--> 363\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m         log_fn(logs)\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining complete\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/music_decomposition/musiclm-pytorch/musiclm_pytorch/trainer.py:324\u001b[0m, in \u001b[0;36mMuLaNTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_every):\n\u001b[1;32m    322\u001b[0m     data_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_tuple_to_kwargs(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl_iter))\n\u001b[0;32m--> 324\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_every)\n\u001b[1;32m    328\u001b[0m     accum_log(logs, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_every})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<@beartype(musiclm_pytorch.musiclm_pytorch.MuLaN.forward) at 0x7f0d40468dc0>:47\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(__beartype_object_7652640, __beartype_getrandbits, __beartype_get_violation, __beartype_conf, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/music_decomposition/musiclm-pytorch/musiclm_pytorch/musiclm_pytorch.py:780\u001b[0m, in \u001b[0;36mMuLaN.forward\u001b[0;34m(self, wavs, texts, raw_texts, return_latents, return_similarities, return_pairwise_similarities)\u001b[0m\n\u001b[1;32m    777\u001b[0m batch, device \u001b[38;5;241m=\u001b[39m wavs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], wavs\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    779\u001b[0m audio_latents, audio_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_audio_latents(wavs, return_all_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 780\u001b[0m text_latents, text_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_latents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_texts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mraw_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_latents:\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m audio_latents, text_latents\n",
      "File \u001b[0;32m<@beartype(musiclm_pytorch.musiclm_pytorch.MuLaN.get_text_latents) at 0x7f0d40468c10>:47\u001b[0m, in \u001b[0;36mget_text_latents\u001b[0;34m(__beartype_object_7652640, __beartype_getrandbits, __beartype_get_violation, __beartype_conf, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/music_decomposition/musiclm-pytorch/musiclm_pytorch/musiclm_pytorch.py:758\u001b[0m, in \u001b[0;36mMuLaN.get_text_latents\u001b[0;34m(self, texts, raw_texts, return_all_layers)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;129m@beartype\u001b[39m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_latents\u001b[39m(\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    756\u001b[0m     return_all_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    757\u001b[0m ):\n\u001b[0;32m--> 758\u001b[0m     text_embeds, text_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_texts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mraw_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m     text_latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_to_latents(text_embeds)\n\u001b[1;32m    760\u001b[0m     out \u001b[38;5;241m=\u001b[39m l2norm(text_latents)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<@beartype(musiclm_pytorch.musiclm_pytorch.TextTransformer.forward) at 0x7f0d40468700>:47\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(__beartype_object_7652640, __beartype_getrandbits, __beartype_get_violation, __beartype_conf, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/music_decomposition/musiclm-pytorch/musiclm_pytorch/musiclm_pytorch.py:630\u001b[0m, in \u001b[0;36mTextTransformer.forward\u001b[0;34m(self, x, raw_texts, mask, return_all_layers)\u001b[0m\n\u001b[1;32m    626\u001b[0m mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(mask, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;66;03m# attention\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m x, all_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# unpack the cls tokens\u001b[39;00m\n\u001b[1;32m    634\u001b[0m cls_tokens, _ \u001b[38;5;241m=\u001b[39m unpack(x, ps, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb * d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/music_decomposition/musiclm-pytorch/musiclm_pytorch/musiclm_pytorch.py:246\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, rel_pos_bias, mask, return_all_layers)\u001b[0m\n\u001b[1;32m    243\u001b[0m layers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attn, ff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 246\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_pos_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    247\u001b[0m     x \u001b[38;5;241m=\u001b[39m ff(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    248\u001b[0m     layers\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/music_decomposition/musiclm-pytorch/musiclm_pytorch/musiclm_pytorch.py:203\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, rel_pos_bias, mask)\u001b[0m\n\u001b[1;32m    199\u001b[0m     sim \u001b[38;5;241m=\u001b[39m sim\u001b[38;5;241m.\u001b[39mmasked_fill(causal_mask, \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfinfo(sim\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# attention\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_dropout(attn)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# aggregate\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mulan_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved mulan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulan_trainer = MuLaNTrainer(mulan=mulan, dataset=training_data, num_train_steps=1000, batch_size=2, grad_accum_every=16)\n",
    "min_loss_path = '/root/musiclm-pytorch/results/mulan_min_loss.pt'\n",
    "mulan_trainer.load(min_loss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most sim music for the given text description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "special_characters = {'&', ',', '\"', \"'\", '/', ';', '', '(', '', '', '.', ')', '-', '\\n', ':'}\n",
    "def replace_special_characters_with_space(text):\n",
    "    for char in special_characters:\n",
    "        text = text.replace(char, ' ')\n",
    "    return text\n",
    "# input\n",
    "query_text = ['This music features a classic piano solo, showcasing intricate melodies and expressive harmonies. The timeless elegance and nuanced performance create an immersive and captivating listening experience.']\n",
    "\n",
    "query_text = [replace_special_characters_with_space(text) for text in query_text]\n",
    "\n",
    "\n",
    "# get the latent representation of the query text\n",
    "query_text_latent = mulan_trainer.mulan.get_text_latents(raw_texts=query_text)\n",
    "\n",
    "# get the audio representation of the query text, highest similarity, iterate over all mulan dataset\n",
    "max_similarity = 0\n",
    "max_similarity_idx = 0\n",
    "idx_simliarity_text_list = []\n",
    "for idx in tqdm(range(len(training_data))):\n",
    "    wav, txt = training_data[idx]\n",
    "    # append fake batch\n",
    "    wav = torch.unsqueeze(wav, 0).to(mulan_trainer.device)\n",
    "    audio_latent = mulan_trainer.mulan.get_audio_latents(wav)\n",
    "    \n",
    "    # compute cosine similarity between two latents\n",
    "    similarity = torch.nn.functional.cosine_similarity(query_text_latent, audio_latent).detach().cpu().numpy()\n",
    "    \n",
    "    idx_simliarity_text_list.append((idx, similarity, txt))\n",
    "    \n",
    "    if similarity > max_similarity:\n",
    "        max_similarity = similarity\n",
    "        max_similarity_idx = idx\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from IPython.display import display\n",
    "\n",
    "# sort the similarity_text list by similarity (higest to lowest)\n",
    "print(query_text)\n",
    "idx_simliarity_text_list.sort(key=lambda x: x[1], reverse=True)\n",
    "for sim_text in idx_simliarity_text_list[:10]:\n",
    "    idx, sim, txt = sim_text\n",
    "    print(f'{idx} {sim} - {txt}')\n",
    "    # display(ipd.Audio(training_data[idx][0], rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the best match\n",
    "print(training_data[max_similarity_idx][1])\n",
    "# play the audio of the best match\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(training_data[max_similarity_idx][0], rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "text1 = ['his voice.  song']\n",
    "text2 = ['his voice   \\n  song']\n",
    "text3 = ['his voice song']\n",
    "\n",
    "embed1 = mulan.get_text_latents(raw_texts=text1)\n",
    "print(embed1.shape)\n",
    "embed2 = mulan.get_text_latents(raw_texts=text2)\n",
    "print(embed2.shape)\n",
    "embed3 = mulan.get_text_latents(raw_texts=text3)\n",
    "\n",
    "print(embed1.sum())\n",
    "print(embed2.sum())\n",
    "print(embed3.sum())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
