{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Download \"music\" portiion of audioset dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir audioset_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioset_download import Downloader\n",
    "d = Downloader(root_path='/mnt/sdb/audioset-download', labels=[\"Music\"], n_jobs=36, download_type='eval', copy_and_replicate=False)\n",
    "d.download(format = 'wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inpsect how many files from 'balanced dataset' are in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "AUSIOSET_PATH = Path('/mnt/shared/alpaca/audioset-download')\n",
    "AUDIOCAP_PATH = Path('musiccaps-public.csv') # you can download this from kaggle\n",
    "\n",
    "ytid_filename_dict = {file.stem[:11]: file for file in AUSIOSET_PATH.rglob('*') if file.is_file() and file.suffix == '.wav'}\n",
    "# make filename_list a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "caption_df_original = pd.read_csv(AUDIOCAP_PATH)\n",
    "\n",
    "caption_df_filtered = caption_df_original[caption_df_original['ytid'].isin(ytid_filename_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(caption_df_original), len(caption_df_filtered)\n",
    "# (5521, 4367), about 80% of the captions are in the audio set 'balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytid_caption_dict = caption_df_filtered.set_index('ytid')['caption'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if ytid are all unique -> yes\n",
    "len(ytid_caption_dict), len(set(ytid_caption_dict.keys()))\n",
    "\n",
    "# check the maximum word length of the captions (not the number of alphabet) -> 136\n",
    "max([len(caption.split()) for caption in ytid_caption_dict.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. save audio as tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mkdir wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the wav file using torchaudio and save it as 16khz in the wavs folder\n",
    "\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "# count the number of sr using defaultdict\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "sr_counter = defaultdict(int)\n",
    "\n",
    "cnt = 0\n",
    "error_keys = []\n",
    "\n",
    "# iterate over the ytid_caption_dict\n",
    "# 1. if the audio cannot be loaded, add the ytid to the error_keys\n",
    "# 2. resample the audio to 16kHz and save it as tensor in the wavs folder\n",
    "# 3. if the audio is shorter than 9 seconds (16000 * 9), discard the audio and add the ytid to the error_keys\n",
    "# 4. clip if the audio is longer than 10 seconds and pad if the audio is shorter than 10 seconds\n",
    "\n",
    "for ytid in tqdm(ytid_caption_dict.keys()):\n",
    "    # catch the decode error\n",
    "    try:\n",
    "        wav_path = ytid_filename_dict[ytid]\n",
    "        wav, sr = torchaudio.load(ytid_filename_dict[ytid])\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_keys.append(ytid)\n",
    "        cnt += 1\n",
    "        continue\n",
    "    \n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.transforms.Resample(sr, 16000)(wav)\n",
    "        \n",
    "    sr = 16000\n",
    "    \n",
    "    audio_len = wav.size(1) / sr\n",
    "    \n",
    "    if audio_len < 9:\n",
    "        error_keys.append(ytid)\n",
    "        cnt += 1\n",
    "        continue\n",
    "    \n",
    "    elif audio_len > 10:\n",
    "        wav = wav[:, :(sr * 10)]\n",
    "    elif audio_len < 10:\n",
    "        # zero pad\n",
    "        wav = torch.nn.functional.pad(wav, (0, (sr * 10) - wav.size(1)))\n",
    "    \n",
    "    # reduce dim to mono\n",
    "    wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "    \n",
    "        \n",
    "    assert wav.size(1) == (sr * 10)\n",
    "    \n",
    "    torchaudio.save(Path(f'wavs/{ytid}.wav'), wav, sr)\n",
    "    sr_counter[sr] += 1\n",
    "    \n",
    "print(cnt)\n",
    "print(sr_counter)\n",
    "\n",
    "ytid_caption_dict = {key: value for key, value in ytid_caption_dict.items() if key not in error_keys}\n",
    "\n",
    "import pickle\n",
    "# save dict\n",
    "with open('ytid_caption_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(ytid_caption_dict, f)\n",
    "\n",
    "# 680 audio files are not available -> 3687"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. filter captions\n",
    "some descriptions include special tokens like ',', '/', '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the non-alphabet characters in caption\n",
    "import pickle\n",
    "\n",
    "with open('ytid_caption_dict.pkl', 'rb') as f:\n",
    "    ytid_caption_dict = pickle.load(f)\n",
    "    \n",
    "special_tokens = set()\n",
    "for caption in ytid_caption_dict.values():\n",
    "    is_bad_caption = False\n",
    "    for char in caption:\n",
    "        if not char.isalnum():\n",
    "            special_tokens.add(char)\n",
    "            is_bad_caption = True\n",
    "    # if is_bad_caption == True:\n",
    "    #     print(caption)\n",
    "            \n",
    "print(special_tokens) # output: {'&', ',', '\"', \"'\", '/', ';', '“', '(', '‘', '’', '.', ')', '-', '\\n', ':', ' '}\n",
    "# musiclm_pytorch (x_clip)'s text encoder only filters ' ', 'tab', '\\n'\n",
    "\n",
    "# remove the special tokens in the dictionary (except for ' ')\n",
    "ytid_caption_dict = {key: ''.join([char for char in value if char.isalnum() or char == ' ']) for key, value in ytid_caption_dict.items()}\n",
    "\n",
    "# save dict\n",
    "with open('ytid_caption_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(ytid_caption_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir preprocessed_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total tensor size of audio files in GB\n",
    "# ensure all the wav files have the same length of 16000 * 10\n",
    "\n",
    "# save the dataset as pickle file for later use\n",
    "\n",
    "import pickle\n",
    "\n",
    "# import defaultdict\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "ytid_caption_dict = None\n",
    "# load saved ytid -> caption dict\n",
    "with open('ytid_caption_dict.pkl', 'rb') as f:\n",
    "    ytid_caption_dict = pickle.load(f)\n",
    "    \n",
    "\n",
    "total_size_in_bytes = 0\n",
    "\n",
    "# counter of the wav length\n",
    "counter_wav_len = defaultdict(int)\n",
    "shape_wave_len = set()\n",
    "\n",
    "wav_list = []\n",
    "txt_list = []\n",
    "\n",
    "\n",
    "for idx, ytid in tqdm(enumerate(sorted(ytid_caption_dict.keys()))):\n",
    "    resampled_wav_path = Path(f'wavs/{ytid}.wav')\n",
    "    wav, sr = torchaudio.load(resampled_wav_path)\n",
    "    wav = wav.squeeze()\n",
    "    assert sr == 16000\n",
    "    counter_wav_len[wav.numel()] += 1\n",
    "    total_size_in_bytes += wav.numel() * wav.element_size()\n",
    "    \n",
    "    # save the wav as tensor.pt\n",
    "    output_no = str(idx).zfill(7)\n",
    "    output_file = Path(f'preprocessed_tensors/{output_no}.pt')\n",
    "    torch.save(wav, output_file)               \n",
    "    \n",
    "    wav_list.append(wav)\n",
    "    txt_list.append(ytid_caption_dict[ytid])\n",
    "\n",
    "total_size_in_gb = total_size_in_bytes / 1024**3\n",
    "print(total_size_in_gb) # 3K = 2.5GB -> 10GB = 12K, 100GB = 120K\n",
    "print(counter_wav_len)\n",
    "\n",
    "wav_list = torch.stack(wav_list)\n",
    "\n",
    "with open('pkls/wavs.pkl', 'wb') as f:\n",
    "    pickle.dump(wav_list, f)\n",
    "    print('wavs.pkl saved')\n",
    "    \n",
    "with open('pkls/txts.pkl', 'wb') as f:\n",
    "    pickle.dump(txt_list, f)\n",
    "    print('txts.pkl saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader for mulan\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MuLanDataset(Dataset):\n",
    "    def __init__(self, audio_pt_path: Path, txt_pickle_path: Path, wav_pickle_path: Path):\n",
    "        \n",
    "        self.audio_pt_path = audio_pt_path\n",
    "        \n",
    "        with open(wav_pickle_path, 'rb') as f:\n",
    "            self.wavs = pickle.load(f)\n",
    "        \n",
    "        with open(txt_pickle_path, 'rb') as f:\n",
    "            self.txts = pickle.load(f)\n",
    "\n",
    "        self.num_data = len(self.txts)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # read wav from pt file, read txt from list\n",
    "        return self.wavs[idx], self.txts[idx]\n",
    "    \n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     # read wav from pt file, read txt from list\n",
    "    #     wav = torch.load(self.audio_pt_path / f'{idx}.pt')\n",
    "    #     txt = self.txts[idx]\n",
    "    #     return wav, txt\n",
    "    \n",
    "    \n",
    "training_data = MuLanDataset(audio_pt_path=Path('preprocessed_tensors'), \n",
    "                             txt_pickle_path=Path('pkls/txts.pkl'),\n",
    "                             wav_pickle_path=Path('pkls/wavs.pkl'))\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, txt = next(iter(train_dataloader))\n",
    "print(wav.shape)\n",
    "print(len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
