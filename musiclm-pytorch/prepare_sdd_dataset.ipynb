{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inpsect data integrity in sdd dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "AUSIOSET_PATH = Path('/mnt/shared/alpaca/song_describer_dataset/audio')\n",
    "AUDIOCAP_PATH = Path('/mnt/shared/alpaca/song_describer_dataset/song_describer.csv') # you can download this from kaggle\n",
    "\n",
    "# check whether there is missing audio files\n",
    "sdd_fileid_set = {str(file.stem[:-5]) for file in AUSIOSET_PATH.rglob('*') if file.is_file() and file.suffix == '.mp3'}\n",
    "sdd_filepath_set = {file for file in AUSIOSET_PATH.rglob('*') if file.is_file() and file.suffix == '.mp3'}\n",
    "# make filename_list a dict\n",
    "\n",
    "import pandas as pd\n",
    "caption_df_original = pd.read_csv(AUDIOCAP_PATH)\n",
    "\n",
    "# extract track ids from caption_df_original and conver to set\n",
    "caption_track_id_set = set(str(x) for x in caption_df_original['track_id'])\n",
    "print(caption_track_id_set)\n",
    "\n",
    "# compare the two sets\n",
    "missing_set = caption_track_id_set - sdd_fileid_set\n",
    "print(len(missing_set)) # OK every audio files are in the csv and vice versa\n",
    "\n",
    "# create a dictionary, key = track_id, value = caption\n",
    "trackid_caption_dict = {str(row['track_id']): row['caption'] for index, row in caption_df_original.iterrows()}\n",
    "print(trackid_caption_dict)\n",
    "\n",
    "# filter some special characters (substitute with space)\n",
    "special_chars = {'&', ',', '\"', \"'\", '/', ';', '“', '(', '‘', '’', '.', ')', '-', '\\n', ':'}\n",
    "track_ids = trackid_caption_dict.keys()\n",
    "for track_id in track_ids:\n",
    "    caption = trackid_caption_dict[track_id]\n",
    "    for char in special_chars:\n",
    "        caption = caption.replace(char, ' ')\n",
    "    trackid_caption_dict[track_id] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os\n",
    "\n",
    "# Function to load and process each file\n",
    "def process_file(file):\n",
    "    wav, sr = torchaudio.load(file)  # Load the audio file\n",
    "    shape = wav.shape\n",
    "\n",
    "    return file, shape, sr\n",
    "\n",
    "# Main function to process files in parallel\n",
    "def process_files_parallel(file_set):\n",
    "    shape_dict = {}\n",
    "    sr_dict = {}\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        results = list(tqdm(executor.map(process_file, file_set), total=len(file_set)))\n",
    "\n",
    "    for result in results:\n",
    "        file, shape, sr = result\n",
    "        shape_dict[file] = shape\n",
    "        sr_dict[file] = sr\n",
    "\n",
    "    return shape_dict, sr_dict\n",
    "\n",
    "shape_dict, sr_dict = process_files_parallel(sdd_filepath_set)\n",
    "print(shape_dict)\n",
    "print(sr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape_dict)\n",
    "# find the audio file of minimum length\n",
    "min_length = min(shape_dict, key=shape_dict.get)\n",
    "print(min_length, shape_dict[min_length]) # it's about 30 seconds...\n",
    "max_length = max(shape_dict, key=shape_dict.get)\n",
    "print(max_length, shape_dict[max_length]) # it's about 2 minutes...\n",
    "\n",
    "# find the total length of the audio files\n",
    "total_length = 0\n",
    "for key in shape_dict:\n",
    "    total_length += (shape_dict[key][1] / sr_dict[key])\n",
    "    \n",
    "total_length_in_hrs = total_length / 3600\n",
    "print(total_length_in_hrs) # 23 hours\n",
    "\n",
    "# padd all the torchaudio to the max length\n",
    "WAV_MAX_LEN = 5759471"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The audios have all the different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "# Function to load and process each file\n",
    "# wav_caption_list = []\n",
    "\n",
    "# for file in sdd_filepath_set:\n",
    "#     wav, sr = torchaudio.load(file)  # Load the audio file\n",
    "#     wav = wav.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "#     wav = torchaudio.transforms.Resample(sr, 16000)(wav)  # Resample to 16kHz\n",
    "#     wav = wav.squeeze()\n",
    "#     # make sure that wav is longer than 10 seconds\n",
    "#     assert wav.shape[0] > 160000\n",
    "    \n",
    "#     # get the caption of the audio file\n",
    "#     file_id = file.stem[:-5]\n",
    "#     # extract the caption for corresponding track_id\n",
    "#     caption = trackid_caption_dict[file_id]\n",
    "#     wav_caption_list.append((wav, caption))\n",
    "#     assert False\n",
    "\n",
    "# Function to load and process each file\n",
    "def process_file(file):\n",
    "    wav, sr = torchaudio.load(file)  # Load the audio file\n",
    "    wav = wav.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "    wav = torchaudio.transforms.Resample(sr, 16000)(wav)  # Resample to 16kHz\n",
    "    wav = wav.squeeze()\n",
    "    # make sure that wav is longer than 10 seconds\n",
    "    assert wav.shape[0] > 160000\n",
    "    \n",
    "    wavlen = wav.shape[0]\n",
    "    # pad the wav to the WAV_MAX_LEN\n",
    "    wav = torch.nn.functional.pad(wav, (0, WAV_MAX_LEN - wavlen))\n",
    "    \n",
    "    \n",
    "    # get the caption of the audio file\n",
    "    file_id = file.stem[:-5]\n",
    "    # extract the caption for corresponding track_id\n",
    "    caption = trackid_caption_dict[file_id]\n",
    "    \n",
    "    \n",
    "    return (wav, caption, wavlen)\n",
    "\n",
    "# Main function to process files in parallel\n",
    "def process_files_parallel(file_set):\n",
    "\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        wav_caption_wavlen_list = list(tqdm(executor.map(process_file, file_set), total=len(file_set)))\n",
    "    \n",
    "    return wav_caption_wavlen_list\n",
    "\n",
    "wav_caption_wavlen_list = process_files_parallel(sdd_filepath_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed data and caption in pickle format separately\n",
    "import pickle\n",
    "\n",
    "with open('pkls/sdd_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(wav_caption_wavlen_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataloder\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class SDDDataset(Dataset):\n",
    "    def __init__(self, sdd_dataset_pkl_path: Path):\n",
    "        \n",
    "        sdd_dataset = pickle.load(open(sdd_dataset_pkl_path, 'rb'))\n",
    "        self.wavs = [x[0] for x in sdd_dataset]\n",
    "        self.txts = [x[1] for x in sdd_dataset]\n",
    "        self.wavlens = [x[2] for x in sdd_dataset]\n",
    "        self.num_data = len(self.wavs)\n",
    "        self.wav_duration = 16000 * 10 # 10 seconds\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # read wav from pt file, read txt from list\n",
    "        real_wav_len = self.wavlens[idx]\n",
    "        # randomly select a starting point\n",
    "        start_point = torch.randint(0, real_wav_len - self.wav_duration, (1,)).item()\n",
    "        wav = self.wavs[idx][start_point:start_point+self.wav_duration]\n",
    "        cap = self.txts[idx]\n",
    "        return wav, cap\n",
    "    \n",
    "    \n",
    "training_data = SDDDataset(sdd_dataset_pkl_path='pkls/sdd_dataset.pkl')\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, txt = next(iter(train_dataloader))\n",
    "for w, t, in zip(wav, txt):\n",
    "    print(w.shape, t)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
