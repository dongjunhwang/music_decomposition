{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environment variable to specify which GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-26 12:04:16.358537: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-26 12:04:16.360276: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-26 12:04:16.397169: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-26 12:04:17.355214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from musiclm_pytorch import MuLaNTrainer, MuLaN\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transformer = AudioSpectrogramTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64,\n",
    "    spec_n_fft = 256,\n",
    "    spec_win_length = 24,\n",
    "    spec_aug_stretch_factor = 0.8\n",
    ")\n",
    "\n",
    "text_transformer = TextTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64\n",
    ")\n",
    "\n",
    "mulan = MuLaN(\n",
    "    audio_transformer = audio_transformer,\n",
    "    text_transformer = text_transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "class MuLanDataset(Dataset):\n",
    "    def __init__(self, txt_pickle_path: Path, wav_pickle_path: Path):\n",
    "                \n",
    "        with open(wav_pickle_path, 'rb') as f:\n",
    "            self.wavs = pickle.load(f)\n",
    "        \n",
    "        with open(txt_pickle_path, 'rb') as f:\n",
    "            self.txts = pickle.load(f)\n",
    "\n",
    "        self.num_data = len(self.txts)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # read wav from pt file, read txt from list\n",
    "        return self.wavs[idx], self.txts[idx]\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, musiccap_txt_pickle_path: Path, musiccap_wav_pickle_path: Path, sdd_dataset_pkl_path: Path):\n",
    "                \n",
    "        with open(musiccap_wav_pickle_path, 'rb') as f:\n",
    "            self.musiccap_wavs = pickle.load(f)\n",
    "        \n",
    "        with open(musiccap_txt_pickle_path, 'rb') as f:\n",
    "            self.musiccaps_txts = pickle.load(f)\n",
    "\n",
    "        self.num_musiccaps = len(self.musiccap_wavs)\n",
    "\n",
    "        sdd_dataset = pickle.load(open(sdd_dataset_pkl_path, 'rb'))\n",
    "        self.sdd_wavs = [x[0] for x in sdd_dataset]\n",
    "        self.sdd_txts = [x[1] for x in sdd_dataset]\n",
    "        self.sdd_wavlens = [x[2] for x in sdd_dataset]\n",
    "        \n",
    "        assert len(self.sdd_wavs) == len(self.sdd_txts) and len(self.sdd_wavlens) == len(self.sdd_txts)\n",
    "        \n",
    "        self.num_sdd = len(self.sdd_wavs)\n",
    "        self.wav_duration = 16000 * 10 # 10 seconds\n",
    "        \n",
    "        self.num_data = len(self.musiccaps_txts) + len(self.sdd_txts)\n",
    "        assert self.num_data == (self.num_musiccaps + self.num_sdd)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.musiccaps_txts):\n",
    "            return self.musiccap_wavs[idx], self.musiccaps_txts[idx]\n",
    "        \n",
    "        else:\n",
    "            # get the index\n",
    "            idx = idx - len(self.musiccaps_txts)\n",
    "            real_wav_len = self.sdd_wavlens[idx]\n",
    "            # randomly select a starting point\n",
    "            start_point = torch.randint(0, real_wav_len - self.wav_duration, (1,)).item()\n",
    "            wav = self.sdd_wavs[idx][start_point:start_point+self.wav_duration]\n",
    "            cap = self.sdd_txts[idx]\n",
    "            return wav, cap\n",
    "    \n",
    "    \n",
    "# training_data = MuLanDataset(\n",
    "#     txt_pickle_path=Path('pkls/txts.pkl'),\n",
    "#     wav_pickle_path=Path('pkls/wavs.pkl'))\n",
    "\n",
    "training_data = MusicDataset(\n",
    "    musiccap_txt_pickle_path=Path('pkls/txts.pkl'),\n",
    "    musiccap_wav_pickle_path=Path('pkls/wavs.pkl'),\n",
    "    sdd_dataset_pkl_path=Path('pkls/sdd_dataset.pkl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3687\n",
      "torch.Size([160000]) This is an alternative rock song with slow tempo and guitar  male vocals and drums \n",
      "torch.Size([160000]) This is an RBsoul music piece There is a male vocalist singing melodically and in a sensual manner over multiple tracks The melody of the beat is provided by a string sample There is a strong bass sound in the piece The rhythm is provided by a groovy electronic drum beat There is an urban feel to this piece It could be used in the soundtrack of a TV series where two characters are sharing intimate moments\n"
     ]
    }
   ],
   "source": [
    "# test MusicDataset\n",
    "num_musiccap_data = training_data.num_musiccaps\n",
    "print(num_musiccap_data)\n",
    "wav, txt = training_data[num_musiccap_data + 10]\n",
    "print(wav.shape, txt)\n",
    "\n",
    "wav, txt = training_data[10]\n",
    "print(wav.shape, txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Mulan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 4173 samples and validating with randomly splitted 220 samples\n"
     ]
    }
   ],
   "source": [
    "mulan_trainer = MuLaNTrainer(mulan=mulan, dataset=training_data, num_train_steps=1000, batch_size=16, grad_accum_every=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectrogram yielded shape of (129, 1251), but had to be cropped to (128, 1248) to be patchified for transformer\n",
      "0: loss: 2.7250650376081467\n",
      "0: saving model to results\n",
      "0: saving model with minimum loss to results\n",
      "1: loss: 3.680168643593788\n",
      "2: loss: 2.72372467815876\n",
      "2: saving model with minimum loss to results\n",
      "3: loss: 2.7109251022338867\n",
      "3: saving model with minimum loss to results\n",
      "4: loss: 2.7091198414564133\n",
      "4: saving model with minimum loss to results\n",
      "5: loss: 2.707549527287483\n",
      "5: saving model with minimum loss to results\n",
      "6: loss: 2.7066099643707275\n",
      "6: saving model with minimum loss to results\n",
      "7: loss: 2.7059807032346725\n",
      "7: saving model with minimum loss to results\n",
      "8: loss: 2.7045385241508484\n",
      "8: saving model with minimum loss to results\n",
      "9: loss: 2.705275133252144\n",
      "10: loss: 2.7030062675476074\n",
      "10: saving model with minimum loss to results\n",
      "11: loss: 2.7012705951929092\n",
      "11: saving model with minimum loss to results\n",
      "12: loss: 2.699175074696541\n",
      "12: saving model with minimum loss to results\n",
      "13: loss: 2.6943281143903732\n",
      "13: saving model with minimum loss to results\n",
      "14: loss: 2.6812456101179123\n",
      "14: saving model with minimum loss to results\n",
      "15: loss: 2.65728497505188\n",
      "15: saving model with minimum loss to results\n",
      "16: loss: 2.687170535326004\n",
      "17: loss: 2.5744381844997406\n",
      "17: saving model with minimum loss to results\n",
      "18: loss: 2.6873791813850403\n",
      "19: loss: 2.6536735445261\n",
      "20: loss: 2.5402365773916245\n",
      "20: saving model with minimum loss to results\n",
      "21: loss: 2.6954429149627686\n",
      "22: loss: 2.616289883852005\n",
      "23: loss: 2.568712145090103\n",
      "24: loss: 2.6101122945547104\n",
      "25: loss: 2.557145267724991\n",
      "26: loss: 2.53220297396183\n",
      "26: saving model with minimum loss to results\n",
      "27: loss: 2.506012573838234\n",
      "27: saving model with minimum loss to results\n",
      "28: loss: 2.4993282109498978\n",
      "28: saving model with minimum loss to results\n",
      "29: loss: 2.5532814264297485\n",
      "30: loss: 2.550610512495041\n",
      "31: loss: 2.5450750291347504\n",
      "32: loss: 2.475625202059746\n",
      "32: saving model with minimum loss to results\n",
      "33: loss: 2.419568046927452\n",
      "33: saving model with minimum loss to results\n",
      "34: loss: 2.418942615389824\n",
      "34: saving model with minimum loss to results\n",
      "35: loss: 2.3878381699323654\n",
      "35: saving model with minimum loss to results\n",
      "36: loss: 2.3403699845075607\n",
      "36: saving model with minimum loss to results\n",
      "37: loss: 2.4856700152158737\n",
      "38: loss: 2.4489814043045044\n",
      "39: loss: 2.4005204886198044\n",
      "40: loss: 2.3746594041585922\n",
      "41: loss: 2.3614373356103897\n",
      "42: loss: 2.426959678530693\n",
      "43: loss: 2.4118562787771225\n",
      "44: loss: 2.3042822405695915\n",
      "44: saving model with minimum loss to results\n",
      "45: loss: 2.343133717775345\n",
      "46: loss: 2.3033704608678818\n",
      "46: saving model with minimum loss to results\n",
      "47: loss: 2.414473682641983\n",
      "48: loss: 2.429015949368477\n",
      "49: loss: 2.2512255162000656\n",
      "49: saving model with minimum loss to results\n",
      "50: loss: 2.289828509092331\n",
      "51: loss: 2.2494311779737473\n",
      "51: saving model with minimum loss to results\n",
      "52: loss: 2.25286103785038\n",
      "53: loss: 2.2591575533151627\n",
      "54: loss: 2.299386888742447\n",
      "55: loss: 2.2126620411872864\n",
      "55: saving model with minimum loss to results\n",
      "56: loss: 2.2003322392702103\n",
      "56: saving model with minimum loss to results\n",
      "57: loss: 2.214382588863373\n",
      "58: loss: 2.2430781051516533\n",
      "59: loss: 2.262350745499134\n",
      "60: loss: 2.2524923235177994\n",
      "61: loss: 2.1525928378105164\n",
      "61: saving model with minimum loss to results\n",
      "62: loss: 2.1609461307525635\n",
      "63: loss: 2.130012035369873\n",
      "63: saving model with minimum loss to results\n",
      "64: loss: 2.1637051850557327\n",
      "65: loss: 2.137697607278824\n",
      "66: loss: 2.1619900465011597\n",
      "67: loss: 2.056960292160511\n",
      "67: saving model with minimum loss to results\n",
      "68: loss: 2.0557491034269333\n",
      "68: saving model with minimum loss to results\n",
      "69: loss: 2.060701236128807\n",
      "70: loss: 2.092436835169792\n",
      "71: loss: 2.1191200464963913\n",
      "72: loss: 2.068248137831688\n",
      "73: loss: 2.1327828764915466\n",
      "74: loss: 2.0039341524243355\n",
      "74: saving model with minimum loss to results\n",
      "75: loss: 2.0681371837854385\n",
      "76: loss: 2.0674591064453125\n",
      "77: loss: 2.0709942802786827\n",
      "78: loss: 2.0443798154592514\n",
      "79: loss: 1.99584399163723\n",
      "79: saving model with minimum loss to results\n",
      "80: loss: 2.0427445247769356\n",
      "81: loss: 2.001644052565098\n",
      "82: loss: 2.09504546970129\n",
      "83: loss: 1.8956418484449387\n",
      "83: saving model with minimum loss to results\n",
      "84: loss: 1.9821978509426117\n",
      "85: loss: 1.8169957250356674\n",
      "85: saving model with minimum loss to results\n",
      "86: loss: 1.7905585318803787\n",
      "86: saving model with minimum loss to results\n",
      "87: loss: 1.94239953905344\n",
      "88: loss: 1.7585715651512146\n",
      "88: saving model with minimum loss to results\n",
      "89: loss: 2.184215821325779\n",
      "90: loss: 2.2045451402664185\n",
      "91: loss: 1.8978250324726105\n",
      "92: loss: 2.0039658918976784\n",
      "93: loss: 2.086768940091133\n",
      "94: loss: 1.8712333664298058\n",
      "95: loss: 1.822023443877697\n",
      "96: loss: 1.8544550091028214\n",
      "97: loss: 1.799291931092739\n",
      "98: loss: 1.8017298206686974\n",
      "99: loss: 1.7706825509667397\n",
      "100: loss: 1.7832499295473099\n",
      "101: loss: 1.825361780822277\n",
      "102: loss: 1.6177540495991707\n",
      "102: saving model with minimum loss to results\n",
      "103: loss: 1.7452437952160835\n",
      "104: loss: 1.751278467476368\n",
      "105: loss: 1.6400919035077095\n",
      "106: loss: 1.598317712545395\n",
      "106: saving model with minimum loss to results\n",
      "107: loss: 1.7185921370983124\n",
      "108: loss: 1.6034505292773247\n",
      "109: loss: 1.7498663142323494\n",
      "110: loss: 1.6063923314213753\n",
      "111: loss: 1.4252338483929634\n",
      "111: saving model with minimum loss to results\n",
      "112: loss: 1.728532798588276\n",
      "113: loss: 1.447363469749689\n",
      "114: loss: 1.4557939544320107\n",
      "115: loss: 1.5569841712713242\n",
      "116: loss: 1.2961210906505585\n",
      "116: saving model with minimum loss to results\n",
      "117: loss: 1.4199122115969658\n",
      "118: loss: 1.4608230292797089\n",
      "119: loss: 1.4042550772428513\n",
      "120: loss: 1.3585842810571194\n",
      "121: loss: 1.3690477833151817\n",
      "122: loss: 1.3558237105607986\n",
      "123: loss: 1.4512301646173\n",
      "124: loss: 1.4616824686527252\n",
      "125: loss: 1.338645190000534\n",
      "126: loss: 1.4235750436782837\n",
      "127: loss: 1.6503293551504612\n",
      "128: loss: 1.4306134656071663\n",
      "129: loss: 1.4059020727872849\n",
      "130: loss: 1.1564533971250057\n",
      "130: saving model with minimum loss to results\n",
      "131: loss: 1.2360769622027874\n",
      "132: loss: 1.4828353524208069\n",
      "133: loss: 1.4230037778615952\n",
      "134: loss: 1.280581172555685\n",
      "135: loss: 1.1939736902713776\n",
      "136: loss: 1.365869216620922\n",
      "137: loss: 1.2757939286530018\n",
      "138: loss: 1.1772836409509182\n",
      "139: loss: 1.2237652502954006\n",
      "140: loss: 1.117985375225544\n",
      "140: saving model with minimum loss to results\n",
      "141: loss: 1.3048726953566074\n",
      "142: loss: 1.0792116075754166\n",
      "142: saving model with minimum loss to results\n",
      "143: loss: 1.0450442172586918\n",
      "143: saving model with minimum loss to results\n",
      "144: loss: 1.1093606986105442\n",
      "145: loss: 1.2313310764729977\n",
      "146: loss: 0.9072595424950123\n",
      "146: saving model with minimum loss to results\n",
      "147: loss: 1.1199963986873627\n",
      "148: loss: 1.0091334357857704\n",
      "149: loss: 1.1289176680147648\n",
      "150: loss: 1.0181850604712963\n",
      "151: loss: 1.0580478496849537\n",
      "152: loss: 1.0027482025325298\n",
      "153: loss: 1.1177407763898373\n",
      "154: loss: 1.1612076833844185\n",
      "155: loss: 0.9665997494012117\n",
      "156: loss: 1.045079404488206\n",
      "157: loss: 1.4346725344657898\n",
      "158: loss: 1.4183755777776241\n",
      "159: loss: 1.2433242127299309\n",
      "160: loss: 0.9619488716125488\n",
      "161: loss: 1.0346753895282745\n",
      "162: loss: 0.9352385327219963\n",
      "163: loss: 0.7492007222026587\n",
      "163: saving model with minimum loss to results\n",
      "164: loss: 0.8517485279589891\n",
      "165: loss: 0.8258339744061232\n",
      "166: loss: 0.9956370163708925\n",
      "167: loss: 0.8610451072454453\n",
      "168: loss: 0.9960140492767096\n",
      "169: loss: 0.7666279077529907\n",
      "170: loss: 0.9972296953201294\n",
      "171: loss: 0.8740958720445633\n",
      "172: loss: 0.9281146638095379\n",
      "173: loss: 0.9108242653310299\n",
      "174: loss: 0.9524686746299267\n",
      "175: loss: 0.8367042504251003\n",
      "176: loss: 0.9265365563333035\n",
      "177: loss: 1.2036469355225563\n",
      "178: loss: 1.1794035285711288\n",
      "179: loss: 0.7899536024779081\n",
      "180: loss: 0.9006445575505495\n",
      "181: loss: 0.8318034373223782\n",
      "182: loss: 0.7368958927690983\n",
      "182: saving model with minimum loss to results\n",
      "183: loss: 0.6476269718259573\n",
      "183: saving model with minimum loss to results\n",
      "184: loss: 0.7054222021251917\n",
      "185: loss: 0.689133757725358\n",
      "186: loss: 0.9112584888935089\n",
      "187: loss: 0.7503458894789219\n",
      "188: loss: 0.6785239316523075\n",
      "189: loss: 0.8334012217819691\n",
      "190: loss: 0.9562856294214725\n",
      "191: loss: 0.7593737486749887\n",
      "192: loss: 0.6641969922930002\n",
      "193: loss: 0.633404765278101\n",
      "193: saving model with minimum loss to results\n",
      "194: loss: 0.8541115932166576\n",
      "195: loss: 0.7960720229893923\n",
      "196: loss: 0.5969714298844337\n",
      "196: saving model with minimum loss to results\n",
      "197: loss: 0.7436374146491289\n",
      "198: loss: 0.6972590629011393\n",
      "199: loss: 0.7267574220895767\n",
      "200: loss: 0.7633859273046255\n",
      "201: loss: 0.6208816915750504\n",
      "202: loss: 0.6457627266645432\n",
      "203: loss: 0.5579071417450905\n",
      "203: saving model with minimum loss to results\n",
      "204: loss: 0.5849312394857407\n",
      "205: loss: 0.6186877284198999\n",
      "206: loss: 0.9465831760317087\n",
      "207: loss: 0.6352244559675455\n",
      "208: loss: 0.6623641587793827\n",
      "209: loss: 0.49115413054823875\n",
      "209: saving model with minimum loss to results\n",
      "210: loss: 0.618828933686018\n",
      "211: loss: 0.6894008535891771\n",
      "212: loss: 0.5325884241610765\n",
      "213: loss: 0.5531074237078428\n",
      "214: loss: 0.552688742056489\n",
      "215: loss: 0.6597497835755348\n",
      "216: loss: 0.548315741121769\n",
      "217: loss: 0.5844328980892897\n",
      "218: loss: 0.5095678362995386\n",
      "219: loss: 0.530897319316864\n",
      "220: loss: 0.4982596430927515\n",
      "221: loss: 0.7217133417725563\n",
      "222: loss: 0.548194769769907\n",
      "223: loss: 0.5504757203161716\n",
      "224: loss: 0.6281649135053158\n",
      "225: loss: 0.6105070821940899\n",
      "226: loss: 0.4495762921869755\n",
      "226: saving model with minimum loss to results\n",
      "227: loss: 0.4849291015416384\n",
      "228: loss: 0.43294036015868187\n",
      "228: saving model with minimum loss to results\n",
      "229: loss: 0.415613554418087\n",
      "229: saving model with minimum loss to results\n",
      "230: loss: 0.37340331077575684\n",
      "230: saving model with minimum loss to results\n",
      "231: loss: 0.39924683701246977\n",
      "232: loss: 0.4450975116342306\n",
      "233: loss: 0.3309420756995678\n",
      "233: saving model with minimum loss to results\n",
      "234: loss: 0.534771217033267\n",
      "235: loss: 0.4231120590120554\n",
      "236: loss: 0.5138603057712317\n",
      "237: loss: 0.44934763573110104\n",
      "238: loss: 0.545225040987134\n",
      "239: loss: 0.42680648900568485\n",
      "240: loss: 0.41005561128258705\n",
      "241: loss: 0.4512592498213053\n",
      "242: loss: 0.4394250400364399\n",
      "243: loss: 0.5109172482043505\n",
      "244: loss: 0.4951832164078951\n",
      "245: loss: 0.5369529575109482\n",
      "246: loss: 0.44028557650744915\n",
      "247: loss: 0.5346826370805502\n",
      "248: loss: 0.42548980563879013\n",
      "249: loss: 0.2574412487447262\n",
      "249: saving model with minimum loss to results\n",
      "250: loss: 0.2934627588838339\n",
      "251: loss: 0.3798167798668146\n",
      "252: loss: 0.5753867793828249\n",
      "253: loss: 0.5033926479518414\n",
      "254: loss: 0.4759588446468115\n",
      "255: loss: 0.4464639909565449\n",
      "256: loss: 0.5071043279021978\n",
      "257: loss: 0.42361685633659363\n",
      "258: loss: 0.4437034633010626\n",
      "259: loss: 0.5437095072120428\n",
      "260: loss: 0.4938400387763977\n",
      "261: loss: 0.2854337291792035\n",
      "262: loss: 0.28532011806964874\n",
      "263: loss: 0.2688821814954281\n",
      "264: loss: 0.30789404548704624\n",
      "265: loss: 0.29585537128150463\n",
      "266: loss: 0.36270793341100216\n",
      "267: loss: 0.2964672539383173\n",
      "268: loss: 0.4051397293806076\n",
      "269: loss: 0.4827267900109291\n",
      "270: loss: 0.42022010684013367\n",
      "271: loss: 0.5201441757380962\n",
      "272: loss: 0.35635806620121\n",
      "273: loss: 0.40306198224425316\n",
      "274: loss: 0.43862148001790047\n",
      "275: loss: 0.4592199232429266\n",
      "276: loss: 0.4205475701019168\n",
      "277: loss: 0.3240926917642355\n",
      "278: loss: 0.35862583853304386\n",
      "279: loss: 0.31480036675930023\n",
      "280: loss: 0.4692733511328697\n",
      "281: loss: 0.35247028432786465\n",
      "282: loss: 0.23564737383276224\n",
      "282: saving model with minimum loss to results\n",
      "283: loss: 0.30773787945508957\n",
      "284: loss: 0.3124351855367422\n",
      "285: loss: 0.270997891202569\n",
      "286: loss: 0.2043259348720312\n",
      "286: saving model with minimum loss to results\n",
      "287: loss: 0.30706494115293026\n",
      "288: loss: 0.4179349523037672\n",
      "289: loss: 0.3059080448001623\n",
      "290: loss: 0.37248408421874046\n",
      "291: loss: 0.3695501983165741\n",
      "292: loss: 0.396737826988101\n",
      "293: loss: 0.2430952312424779\n",
      "294: loss: 0.33397798240184784\n",
      "295: loss: 0.2714028377085924\n",
      "296: loss: 0.2656952291727066\n",
      "297: loss: 0.35086357221007347\n",
      "298: loss: 0.2918509729206562\n",
      "299: loss: 0.3135555125772953\n",
      "300: loss: 0.357152845710516\n",
      "301: loss: 0.2701682038605213\n",
      "302: loss: 0.18416968919336796\n",
      "302: saving model with minimum loss to results\n",
      "303: loss: 0.33045613393187523\n",
      "304: loss: 0.24612907879054546\n",
      "305: loss: 0.23094375245273113\n",
      "306: loss: 0.34670725278556347\n",
      "307: loss: 0.3633137606084347\n",
      "308: loss: 0.24734766967594624\n",
      "309: loss: 0.21284679882228374\n",
      "310: loss: 0.27990698255598545\n",
      "311: loss: 0.2671211063861847\n",
      "312: loss: 0.2116491300985217\n",
      "313: loss: 0.3434870336204767\n",
      "314: loss: 0.18250416405498981\n",
      "314: saving model with minimum loss to results\n",
      "315: loss: 0.2309498656541109\n",
      "316: loss: 0.2261323146522045\n",
      "317: loss: 0.20924922451376915\n",
      "318: loss: 0.30885125882923603\n",
      "319: loss: 0.27953552454710007\n",
      "320: loss: 0.2853173166513443\n",
      "321: loss: 0.2862212546169758\n",
      "322: loss: 0.2220934983342886\n",
      "323: loss: 0.1718028662726283\n",
      "323: saving model with minimum loss to results\n",
      "324: loss: 0.18613255582749844\n",
      "325: loss: 0.13408828061074018\n",
      "325: saving model with minimum loss to results\n",
      "326: loss: 0.32214599661529064\n",
      "327: loss: 0.41748131066560745\n",
      "328: loss: 0.20888308621942997\n",
      "329: loss: 0.20613072626292706\n",
      "330: loss: 0.25187158212065697\n",
      "331: loss: 0.16285734623670578\n",
      "332: loss: 0.21863908600062132\n",
      "333: loss: 0.2896371465176344\n",
      "334: loss: 0.33421902917325497\n",
      "335: loss: 0.23359432257711887\n",
      "336: loss: 0.3574891220778227\n",
      "337: loss: 0.2841246807947755\n",
      "338: loss: 0.2988016903400421\n",
      "339: loss: 0.24149552546441555\n",
      "340: loss: 0.32343973591923714\n",
      "341: loss: 0.24684242717921734\n",
      "342: loss: 0.1652418114244938\n",
      "343: loss: 0.2010395908728242\n",
      "344: loss: 0.23233589623123407\n",
      "345: loss: 0.16755112074315548\n",
      "346: loss: 0.2556469142436981\n",
      "347: loss: 0.20937871374189854\n",
      "348: loss: 0.1268130224198103\n",
      "348: saving model with minimum loss to results\n",
      "349: loss: 0.3249611109495163\n",
      "350: loss: 0.3224135972559452\n",
      "351: loss: 0.3238910362124443\n",
      "352: loss: 0.19541852083057165\n",
      "353: loss: 0.21190425008535385\n",
      "354: loss: 0.20513700507581234\n",
      "355: loss: 0.346231522038579\n",
      "356: loss: 0.42303289473056793\n",
      "357: loss: 0.17284007743000984\n",
      "358: loss: 0.2571817822754383\n",
      "359: loss: 0.2874839073047042\n",
      "360: loss: 0.2904075216501951\n",
      "361: loss: 0.2445274144411087\n",
      "362: loss: 0.28647911828011274\n",
      "363: loss: 0.3043720666319132\n",
      "364: loss: 0.32442078180611134\n",
      "365: loss: 0.26354342326521873\n",
      "366: loss: 0.20775632746517658\n",
      "367: loss: 0.28460477851331234\n",
      "368: loss: 0.216055516153574\n",
      "369: loss: 0.26589807495474815\n",
      "370: loss: 0.3000647481530905\n",
      "371: loss: 0.2936844080686569\n",
      "372: loss: 0.31981160305440426\n",
      "373: loss: 0.11501078307628632\n",
      "373: saving model with minimum loss to results\n",
      "374: loss: 0.16694360598921776\n",
      "375: loss: 0.24981924518942833\n",
      "376: loss: 0.19111907202750444\n",
      "377: loss: 0.18273760378360748\n",
      "378: loss: 0.30135021917521954\n",
      "379: loss: 0.23140699416399002\n",
      "380: loss: 0.21358068473637104\n",
      "381: loss: 0.38622428104281425\n",
      "382: loss: 0.19391298666596413\n",
      "383: loss: 0.1169141661375761\n",
      "384: loss: 0.17011183220893145\n",
      "385: loss: 0.2511543147265911\n",
      "386: loss: 0.16814819630235434\n",
      "387: loss: 0.19554135389626026\n",
      "388: loss: 0.14550166577100754\n",
      "389: loss: 0.17271062918007374\n",
      "390: loss: 0.17430483642965555\n",
      "391: loss: 0.20557280257344246\n",
      "392: loss: 0.18464467488229275\n",
      "393: loss: 0.1657115202397108\n",
      "394: loss: 0.23915200028568506\n",
      "395: loss: 0.15544970519840717\n",
      "396: loss: 0.22991310246288776\n",
      "397: loss: 0.13712116330862045\n",
      "398: loss: 0.1352283675223589\n",
      "399: loss: 0.15014536958187819\n",
      "400: loss: 0.1692269816994667\n",
      "401: loss: 0.16007724590599537\n",
      "402: loss: 0.23528657667338848\n",
      "403: loss: 0.2605709470808506\n",
      "404: loss: 0.27659914642572403\n",
      "405: loss: 0.18240178748965263\n",
      "406: loss: 0.10416139476001263\n",
      "406: saving model with minimum loss to results\n",
      "407: loss: 0.1448803311213851\n",
      "408: loss: 0.2471275757998228\n",
      "409: loss: 0.23036549985408783\n",
      "410: loss: 0.19107586704194546\n",
      "411: loss: 0.12139521725475788\n",
      "412: loss: 0.17183693777769804\n",
      "413: loss: 0.21596588380634785\n",
      "414: loss: 0.18160440120846033\n",
      "415: loss: 0.210438696667552\n",
      "416: loss: 0.16145137883722782\n",
      "417: loss: 0.2659460976719856\n",
      "418: loss: 0.119212182238698\n",
      "419: loss: 0.16895763855427504\n",
      "420: loss: 0.14511780999600887\n",
      "421: loss: 0.3361420501023531\n",
      "422: loss: 0.2421159539371729\n",
      "423: loss: 0.24585198052227497\n",
      "424: loss: 0.1788235828280449\n",
      "425: loss: 0.13560943491756916\n",
      "426: loss: 0.1716229747980833\n",
      "427: loss: 0.25547781214118004\n",
      "428: loss: 0.16110891290009022\n",
      "429: loss: 0.19694566261023283\n",
      "430: loss: 0.14792065136134624\n",
      "431: loss: 0.2232719585299492\n",
      "432: loss: 0.22508364729583263\n",
      "433: loss: 0.18905842304229736\n",
      "434: loss: 0.28855497390031815\n",
      "435: loss: 0.193639712408185\n",
      "436: loss: 0.17955825105309486\n",
      "437: loss: 0.18930363561958075\n",
      "438: loss: 0.082446264103055\n",
      "438: saving model with minimum loss to results\n",
      "439: loss: 0.14557968359440565\n",
      "440: loss: 0.13222705200314522\n",
      "441: loss: 0.23716113902628422\n",
      "442: loss: 0.15007422491908073\n",
      "443: loss: 0.17594950832426548\n",
      "444: loss: 0.2698737634345889\n",
      "445: loss: 0.11628367658704519\n",
      "446: loss: 0.12076177168637514\n",
      "447: loss: 0.08823725208640099\n",
      "448: loss: 0.13713453710079193\n",
      "449: loss: 0.15032182354480028\n",
      "450: loss: 0.10598826222121716\n",
      "451: loss: 0.1657417668029666\n",
      "452: loss: 0.17907824367284775\n",
      "453: loss: 0.16083108633756638\n",
      "454: loss: 0.11703567020595074\n",
      "455: loss: 0.21478538028895855\n",
      "456: loss: 0.1418284922838211\n",
      "457: loss: 0.18192474357783794\n",
      "458: loss: 0.15218137949705124\n",
      "459: loss: 0.09561121836304665\n",
      "460: loss: 0.14565534330904484\n",
      "461: loss: 0.10796602442860603\n",
      "462: loss: 0.10744543373584747\n",
      "463: loss: 0.25095806643366814\n",
      "464: loss: 0.1789191970601678\n",
      "465: loss: 0.1865346934646368\n",
      "466: loss: 0.20637332554906607\n",
      "467: loss: 0.20798350498080254\n",
      "468: loss: 0.1612963266670704\n",
      "469: loss: 0.18907527439296246\n",
      "470: loss: 0.13997601717710495\n",
      "471: loss: 0.16587981767952442\n",
      "472: loss: 0.12781829945743084\n",
      "473: loss: 0.16802221722900867\n",
      "474: loss: 0.1690378338098526\n",
      "475: loss: 0.2292503770440817\n",
      "476: loss: 0.1910191010683775\n",
      "477: loss: 0.12929780688136816\n",
      "478: loss: 0.17394886538386345\n",
      "479: loss: 0.14782660081982613\n",
      "480: loss: 0.1780484849587083\n",
      "481: loss: 0.14077947940677404\n",
      "482: loss: 0.0948172565549612\n",
      "483: loss: 0.18951434083282948\n",
      "484: loss: 0.13704907521605492\n",
      "485: loss: 0.10766815207898617\n",
      "486: loss: 0.12991702742874622\n",
      "487: loss: 0.11121700331568718\n",
      "488: loss: 0.12022905051708221\n",
      "489: loss: 0.0860321894288063\n",
      "490: loss: 0.16403324343264103\n",
      "491: loss: 0.1169393751770258\n",
      "492: loss: 0.12492850422859192\n",
      "493: loss: 0.13136136531829834\n",
      "494: loss: 0.0977132385596633\n",
      "495: loss: 0.12516813725233078\n",
      "496: loss: 0.13988329283893108\n",
      "497: loss: 0.12993913050740957\n",
      "498: loss: 0.13498041033744812\n",
      "499: loss: 0.1782443057745695\n",
      "500: loss: 0.14294870756566525\n",
      "501: loss: 0.13461732119321823\n",
      "502: loss: 0.18708881922066212\n",
      "503: loss: 0.1493429159745574\n",
      "504: loss: 0.11357514560222626\n",
      "505: loss: 0.1906985668465495\n",
      "506: loss: 0.07400925923138857\n",
      "506: saving model with minimum loss to results\n",
      "507: loss: 0.1502383705228567\n",
      "508: loss: 0.17665464244782925\n",
      "509: loss: 0.09059532359242439\n",
      "510: loss: 0.12642568256706\n",
      "511: loss: 0.11360163148492575\n",
      "512: loss: 0.11599959805607796\n",
      "513: loss: 0.09501694329082966\n",
      "514: loss: 0.08530901093035936\n",
      "515: loss: 0.09696435183286667\n",
      "516: loss: 0.16489463485777378\n",
      "517: loss: 0.1612466163933277\n",
      "518: loss: 0.1284453235566616\n",
      "519: loss: 0.15525458846241236\n",
      "520: loss: 0.16000662744045258\n",
      "521: loss: 0.10227835178375244\n",
      "522: loss: 0.13735496252775192\n",
      "523: loss: 0.1393859749659896\n",
      "524: loss: 0.2077234983444214\n",
      "525: loss: 0.1870174426585436\n",
      "526: loss: 0.11036265641450882\n",
      "527: loss: 0.1173666100949049\n",
      "528: loss: 0.18970610946416855\n",
      "529: loss: 0.12382755428552628\n",
      "530: loss: 0.186219597235322\n",
      "531: loss: 0.19047201052308083\n",
      "532: loss: 0.11982874013483524\n",
      "533: loss: 0.14862973056733608\n",
      "534: loss: 0.10638845339417458\n",
      "535: loss: 0.19526638463139534\n",
      "536: loss: 0.15435242746025324\n",
      "537: loss: 0.18981791101396084\n",
      "538: loss: 0.15884316805750132\n",
      "539: loss: 0.151493058539927\n",
      "540: loss: 0.1380904670804739\n",
      "541: loss: 0.15573927573859692\n",
      "542: loss: 0.14901219215244055\n",
      "543: loss: 0.16935714427381754\n",
      "544: loss: 0.15791174303740263\n",
      "545: loss: 0.11131357494741678\n",
      "546: loss: 0.11493145767599344\n",
      "547: loss: 0.12516622990369797\n",
      "548: loss: 0.13690659776329994\n",
      "549: loss: 0.19174939393997192\n",
      "550: loss: 0.15126195549964905\n",
      "551: loss: 0.17040324490517378\n",
      "552: loss: 0.09375338070094585\n",
      "553: loss: 0.0822762306779623\n",
      "554: loss: 0.17167284153401852\n",
      "555: loss: 0.10071296617388725\n",
      "556: loss: 0.08520797733217478\n",
      "557: loss: 0.13309984933584929\n",
      "558: loss: 0.09980104677379131\n",
      "559: loss: 0.12224592361599207\n",
      "560: loss: 0.24556241184473038\n",
      "561: loss: 0.15064662881195545\n",
      "562: loss: 0.08192074298858643\n",
      "563: loss: 0.07007642649114132\n",
      "563: saving model with minimum loss to results\n",
      "564: loss: 0.0959811881184578\n",
      "565: loss: 0.16521438211202621\n",
      "566: loss: 0.05242755822837353\n",
      "566: saving model with minimum loss to results\n",
      "567: loss: 0.08554991148412228\n",
      "568: loss: 0.142296033911407\n",
      "569: loss: 0.1365173775702715\n",
      "570: loss: 0.15007961355149746\n",
      "571: loss: 0.09434657823294401\n",
      "572: loss: 0.09646281227469444\n",
      "573: loss: 0.1816901694983244\n",
      "574: loss: 0.19776042364537716\n",
      "575: loss: 0.10785387083888054\n",
      "576: loss: 0.13475729152560234\n",
      "577: loss: 0.2659068088978529\n",
      "578: loss: 0.18273177929222584\n",
      "579: loss: 0.1588733084499836\n",
      "580: loss: 0.15867662988603115\n",
      "581: loss: 0.13646491803228855\n",
      "582: loss: 0.11653722077608109\n",
      "583: loss: 0.1346315946429968\n",
      "584: loss: 0.14964880514889956\n",
      "585: loss: 0.08175923675298691\n",
      "586: loss: 0.15982848964631557\n",
      "587: loss: 0.06774805299937725\n",
      "588: loss: 0.13968669343739748\n",
      "589: loss: 0.10966894309967756\n",
      "590: loss: 0.10728158988058567\n",
      "591: loss: 0.0941351605579257\n",
      "592: loss: 0.08600001968443394\n",
      "593: loss: 0.06719433516263962\n",
      "594: loss: 0.16052310355007648\n",
      "595: loss: 0.15494940802454948\n",
      "596: loss: 0.09293131344020367\n",
      "597: loss: 0.20063351839780807\n",
      "598: loss: 0.09080754220485687\n",
      "599: loss: 0.18386851623654366\n",
      "600: loss: 0.1610875427722931\n",
      "601: loss: 0.11524700559675694\n",
      "602: loss: 0.20623769517987967\n",
      "603: loss: 0.08196574822068214\n",
      "604: loss: 0.0909974342212081\n",
      "605: loss: 0.15412098541855812\n",
      "606: loss: 0.09256290644407272\n",
      "607: loss: 0.15824724175035954\n",
      "608: loss: 0.12050263490527868\n",
      "609: loss: 0.17944384645670652\n",
      "610: loss: 0.06654490996152163\n",
      "611: loss: 0.15968230739235878\n",
      "612: loss: 0.12102007120847702\n",
      "613: loss: 0.1283465437591076\n",
      "614: loss: 0.11015324108302593\n",
      "615: loss: 0.051543451845645905\n",
      "615: saving model with minimum loss to results\n",
      "616: loss: 0.14365281350910664\n",
      "617: loss: 0.10000689420849085\n",
      "618: loss: 0.15528849698603153\n",
      "619: loss: 0.06936964951455593\n",
      "620: loss: 0.1330470535904169\n",
      "621: loss: 0.11753535084426403\n",
      "622: loss: 0.11209697276353836\n",
      "623: loss: 0.07226869836449623\n",
      "624: loss: 0.16165958158671856\n",
      "625: loss: 0.107488464564085\n",
      "626: loss: 0.058282047510147095\n",
      "627: loss: 0.1189310122281313\n",
      "628: loss: 0.09712778776884079\n",
      "629: loss: 0.20724577084183693\n",
      "630: loss: 0.11896898970007896\n",
      "631: loss: 0.06492573954164982\n",
      "632: loss: 0.17298266850411892\n",
      "633: loss: 0.11145847663283348\n",
      "634: loss: 0.16023123264312744\n",
      "635: loss: 0.110026097856462\n",
      "636: loss: 0.11257215216755867\n",
      "637: loss: 0.11532377824187279\n",
      "638: loss: 0.11839341185986996\n",
      "639: loss: 0.053609964437782764\n",
      "640: loss: 0.08757183235138655\n",
      "641: loss: 0.13923407346010208\n",
      "642: loss: 0.14978758059442043\n",
      "643: loss: 0.12206469848752022\n",
      "644: loss: 0.11415488086640835\n",
      "645: loss: 0.09864885546267033\n",
      "646: loss: 0.13237318210303783\n",
      "647: loss: 0.14925197884440422\n",
      "648: loss: 0.12682158034294844\n",
      "649: loss: 0.10064234025776386\n",
      "650: loss: 0.09517048671841621\n",
      "651: loss: 0.09548562206327915\n",
      "652: loss: 0.09461973328143358\n",
      "653: loss: 0.07383242156356573\n",
      "654: loss: 0.07168294303119183\n",
      "655: loss: 0.16292363591492176\n",
      "656: loss: 0.08029014337807894\n",
      "657: loss: 0.10676754079759121\n",
      "658: loss: 0.14448759891092777\n",
      "659: loss: 0.175797987729311\n",
      "660: loss: 0.0709212739020586\n",
      "661: loss: 0.0894893566146493\n",
      "662: loss: 0.1077048284932971\n",
      "663: loss: 0.1250281147658825\n",
      "664: loss: 0.06376184709370136\n",
      "665: loss: 0.09498437028378248\n",
      "666: loss: 0.12813833355903625\n",
      "667: loss: 0.1468706000596285\n",
      "668: loss: 0.12641961500048637\n",
      "669: loss: 0.10507836751639843\n",
      "670: loss: 0.116868045181036\n",
      "671: loss: 0.05592129845172167\n",
      "672: loss: 0.13767095655202866\n",
      "673: loss: 0.08444210700690746\n",
      "674: loss: 0.09106955863535404\n",
      "675: loss: 0.15579201094806194\n",
      "676: loss: 0.1274743601679802\n",
      "677: loss: 0.08992231078445911\n",
      "678: loss: 0.1882751677185297\n",
      "679: loss: 0.08769929222762585\n",
      "680: loss: 0.15252156369388103\n",
      "681: loss: 0.09480836056172848\n",
      "682: loss: 0.16802127473056316\n",
      "683: loss: 0.10700313374400139\n",
      "684: loss: 0.10330682061612606\n",
      "685: loss: 0.05971515737473965\n",
      "686: loss: 0.08964507281780243\n",
      "687: loss: 0.09168310277163982\n",
      "688: loss: 0.13921379391103983\n",
      "689: loss: 0.1250705672428012\n",
      "690: loss: 0.11311470065265894\n",
      "691: loss: 0.11792085878551006\n",
      "692: loss: 0.09322918206453323\n",
      "693: loss: 0.07129760272800922\n",
      "694: loss: 0.08098884113132954\n",
      "695: loss: 0.09317715093493462\n",
      "696: loss: 0.07293154392391443\n",
      "697: loss: 0.11812121234834194\n",
      "698: loss: 0.06502932868897915\n",
      "699: loss: 0.11336234770715237\n",
      "700: loss: 0.1757614826783538\n",
      "701: loss: 0.07525387033820152\n",
      "702: loss: 0.14253665134310722\n",
      "703: loss: 0.09672960545867682\n",
      "704: loss: 0.13430223520845175\n",
      "705: loss: 0.12429440580308437\n",
      "706: loss: 0.10176190827041864\n",
      "707: loss: 0.09986345283687115\n",
      "708: loss: 0.06304740160703659\n",
      "709: loss: 0.10316848382353783\n",
      "710: loss: 0.1466931039467454\n",
      "711: loss: 0.0776413232088089\n",
      "712: loss: 0.08236892707645893\n",
      "713: loss: 0.10684298817068338\n",
      "714: loss: 0.16826200298964977\n",
      "715: loss: 0.06795572862029076\n",
      "716: loss: 0.07092228345572948\n",
      "717: loss: 0.06902793608605862\n",
      "718: loss: 0.11360798310488462\n",
      "719: loss: 0.10559508763253689\n",
      "720: loss: 0.09059464931488037\n",
      "721: loss: 0.06892073526978493\n",
      "722: loss: 0.07231623586267233\n",
      "723: loss: 0.11770185455679893\n",
      "724: loss: 0.08284357376396656\n",
      "725: loss: 0.09758944809436798\n",
      "726: loss: 0.14966855570673943\n",
      "727: loss: 0.07848821301013231\n",
      "728: loss: 0.08557991404086351\n",
      "729: loss: 0.17770424857735634\n",
      "730: loss: 0.09456953685730696\n",
      "731: loss: 0.13236909080296755\n",
      "732: loss: 0.11217406578361988\n",
      "733: loss: 0.0873542595654726\n",
      "734: loss: 0.08259881474077702\n",
      "735: loss: 0.08583217859268188\n",
      "736: loss: 0.05716356169432402\n",
      "737: loss: 0.1319719636812806\n",
      "738: loss: 0.05703798681497574\n",
      "739: loss: 0.11609713919460773\n",
      "740: loss: 0.0871580932289362\n",
      "741: loss: 0.15547329001128674\n",
      "742: loss: 0.07017862610518932\n",
      "743: loss: 0.06432080641388893\n",
      "744: loss: 0.056089216843247414\n",
      "745: loss: 0.06241491995751858\n",
      "746: loss: 0.07714927196502686\n",
      "747: loss: 0.09270041435956955\n",
      "748: loss: 0.1429981328547001\n",
      "749: loss: 0.08044422045350075\n",
      "750: loss: 0.07609033584594727\n",
      "751: loss: 0.073883350007236\n",
      "752: loss: 0.077276561409235\n",
      "753: loss: 0.08415242657065392\n",
      "754: loss: 0.06531958095729351\n",
      "755: loss: 0.08934571035206318\n",
      "756: loss: 0.09655319154262543\n",
      "757: loss: 0.09003540128469467\n",
      "758: loss: 0.062022291123867035\n",
      "759: loss: 0.14163734950125217\n",
      "760: loss: 0.09993919637054205\n",
      "761: loss: 0.16468915715813637\n",
      "762: loss: 0.08306396193802357\n",
      "763: loss: 0.11491373553872108\n",
      "764: loss: 0.09088355302810669\n",
      "765: loss: 0.05022425763309002\n",
      "765: saving model with minimum loss to results\n",
      "766: loss: 0.05966953933238983\n",
      "767: loss: 0.08736083097755909\n",
      "768: loss: 0.10235948674380779\n",
      "769: loss: 0.06777654401957989\n",
      "770: loss: 0.12692278623580933\n",
      "771: loss: 0.07926815375685692\n",
      "772: loss: 0.10550504829734564\n",
      "773: loss: 0.11472738906741142\n",
      "774: loss: 0.16417782939970493\n",
      "775: loss: 0.0648243147879839\n",
      "776: loss: 0.0832686685025692\n",
      "777: loss: 0.041864531114697456\n",
      "777: saving model with minimum loss to results\n",
      "778: loss: 0.09934934228658676\n",
      "779: loss: 0.050538680516183376\n",
      "780: loss: 0.07282968983054161\n",
      "781: loss: 0.1540563516318798\n",
      "782: loss: 0.10046098101884127\n",
      "783: loss: 0.06208558939397335\n",
      "784: loss: 0.16075186990201473\n",
      "785: loss: 0.040515922009944916\n",
      "785: saving model with minimum loss to results\n",
      "786: loss: 0.06402458436787128\n",
      "787: loss: 0.13865107484161854\n",
      "788: loss: 0.13483714405447245\n",
      "789: loss: 0.08641922473907471\n",
      "790: loss: 0.06656225491315126\n",
      "791: loss: 0.045454198494553566\n",
      "792: loss: 0.1900499016046524\n",
      "793: loss: 0.07518661953508854\n",
      "794: loss: 0.19074399955570698\n",
      "795: loss: 0.0774709414690733\n",
      "796: loss: 0.1299314545467496\n",
      "797: loss: 0.13147970661520958\n",
      "798: loss: 0.15715299174189568\n",
      "799: loss: 0.04902823641896248\n",
      "800: loss: 0.060947950929403305\n",
      "801: loss: 0.09368093311786652\n",
      "802: loss: 0.07993985339999199\n",
      "803: loss: 0.09177100844681263\n",
      "804: loss: 0.06259638257324696\n",
      "805: loss: 0.089195073582232\n",
      "806: loss: 0.0821266807615757\n",
      "807: loss: 0.1053913738578558\n",
      "808: loss: 0.09075009077787399\n",
      "809: loss: 0.1496989130973816\n",
      "810: loss: 0.10835971683263779\n",
      "811: loss: 0.11035076901316643\n",
      "812: loss: 0.053189339116215706\n",
      "813: loss: 0.17637689225375652\n",
      "814: loss: 0.10872466117143631\n",
      "815: loss: 0.07038893178105354\n",
      "816: loss: 0.059794504195451736\n",
      "817: loss: 0.11965874396264553\n",
      "818: loss: 0.10444576758891344\n",
      "819: loss: 0.07227860949933529\n",
      "820: loss: 0.031927961856126785\n",
      "820: saving model with minimum loss to results\n",
      "821: loss: 0.0710777286440134\n",
      "822: loss: 0.10706508904695511\n",
      "823: loss: 0.05053411237895489\n",
      "824: loss: 0.11073546670377254\n",
      "825: loss: 0.09362574201077223\n",
      "826: loss: 0.040318043902516365\n",
      "827: loss: 0.1019581388682127\n",
      "828: loss: 0.1353760901838541\n",
      "829: loss: 0.029585927724838257\n",
      "829: saving model with minimum loss to results\n",
      "830: loss: 0.05162312649190426\n",
      "831: loss: 0.10100060142576694\n",
      "832: loss: 0.05092066526412964\n",
      "833: loss: 0.08483956009149551\n",
      "834: loss: 0.034880802035331726\n",
      "835: loss: 0.041170258074998856\n",
      "836: loss: 0.06056665629148483\n",
      "837: loss: 0.041855549439787865\n",
      "838: loss: 0.061013391241431236\n",
      "839: loss: 0.13082278333604336\n",
      "840: loss: 0.09032744355499744\n",
      "841: loss: 0.0506519814953208\n",
      "842: loss: 0.07300546206533909\n",
      "843: loss: 0.08120491448789835\n",
      "844: loss: 0.05605894699692726\n",
      "845: loss: 0.0654148031026125\n",
      "846: loss: 0.06202624086290598\n",
      "847: loss: 0.07295058481395245\n",
      "848: loss: 0.09002533927559853\n",
      "849: loss: 0.1233849823474884\n",
      "850: loss: 0.08856858126819134\n",
      "851: loss: 0.10392524302005768\n",
      "852: loss: 0.09040204808115959\n",
      "853: loss: 0.16018244251608849\n",
      "854: loss: 0.10503494925796986\n",
      "855: loss: 0.09678942710161209\n",
      "856: loss: 0.058054687455296516\n",
      "857: loss: 0.09313404001295567\n",
      "858: loss: 0.07494369521737099\n",
      "859: loss: 0.13043934106826782\n",
      "860: loss: 0.07874460518360138\n",
      "861: loss: 0.1670207902789116\n",
      "862: loss: 0.14536502119153738\n",
      "863: loss: 0.07226628251373768\n",
      "864: loss: 0.09200325980782509\n",
      "865: loss: 0.1344739105552435\n",
      "866: loss: 0.10625383257865906\n",
      "867: loss: 0.16550107672810555\n",
      "868: loss: 0.0716576287522912\n",
      "869: loss: 0.10368951596319675\n",
      "870: loss: 0.06130995601415634\n",
      "871: loss: 0.04586432222276926\n",
      "872: loss: 0.051589444279670715\n",
      "873: loss: 0.08818670734763145\n",
      "874: loss: 0.11025124974548817\n",
      "875: loss: 0.051363855600357056\n",
      "876: loss: 0.08772844821214676\n",
      "877: loss: 0.09090912155807018\n",
      "878: loss: 0.05524055473506451\n",
      "879: loss: 0.062307288870215416\n",
      "880: loss: 0.06873451173305511\n",
      "881: loss: 0.11411584168672562\n",
      "882: loss: 0.07401813566684723\n",
      "883: loss: 0.06106116995215416\n",
      "884: loss: 0.0719451867043972\n",
      "885: loss: 0.05433836579322815\n",
      "886: loss: 0.05325452797114849\n",
      "887: loss: 0.06396962329745293\n",
      "888: loss: 0.0897773876786232\n",
      "889: loss: 0.04790378548204899\n",
      "890: loss: 0.07779452484101057\n",
      "891: loss: 0.15754836425185204\n",
      "892: loss: 0.08655898831784725\n",
      "893: loss: 0.06425567902624607\n",
      "894: loss: 0.06635346449911594\n",
      "895: loss: 0.07662999257445335\n",
      "896: loss: 0.034463685005903244\n",
      "897: loss: 0.06769498623907566\n",
      "898: loss: 0.0563071146607399\n",
      "899: loss: 0.04151690565049648\n",
      "900: loss: 0.11275249905884266\n",
      "901: loss: 0.07293209619820118\n",
      "902: loss: 0.06871844828128815\n",
      "903: loss: 0.05206983722746372\n",
      "904: loss: 0.08363988623023033\n",
      "905: loss: 0.024578431621193886\n",
      "905: saving model with minimum loss to results\n",
      "906: loss: 0.09846159815788269\n",
      "907: loss: 0.23483749013394117\n",
      "908: loss: 0.07341463305056095\n",
      "909: loss: 0.15919090621173382\n",
      "910: loss: 0.06379038281738758\n",
      "911: loss: 0.02859247662127018\n",
      "912: loss: 0.09636153373867273\n",
      "913: loss: 0.07750651985406876\n",
      "914: loss: 0.06552025768905878\n",
      "915: loss: 0.08715350925922394\n",
      "916: loss: 0.06679004803299904\n",
      "917: loss: 0.15416619926691055\n",
      "918: loss: 0.07288998179137707\n",
      "919: loss: 0.07565436139702797\n",
      "920: loss: 0.07943451590836048\n",
      "921: loss: 0.06375276111066341\n",
      "922: loss: 0.0604197159409523\n",
      "923: loss: 0.035975901409983635\n",
      "924: loss: 0.08068453520536423\n",
      "925: loss: 0.10067091882228851\n",
      "926: loss: 0.053573260083794594\n",
      "927: loss: 0.06602253392338753\n",
      "928: loss: 0.06354920007288456\n",
      "929: loss: 0.1101255714893341\n",
      "930: loss: 0.12019102182239294\n",
      "931: loss: 0.07131284940987825\n",
      "932: loss: 0.07409347593784332\n",
      "933: loss: 0.05623322073370218\n",
      "934: loss: 0.08219851087778807\n",
      "935: loss: 0.09069052617996931\n",
      "936: loss: 0.038574494421482086\n",
      "937: loss: 0.06301493011415005\n",
      "938: loss: 0.039568422362208366\n",
      "939: loss: 0.056631723418831825\n",
      "940: loss: 0.06250759959220886\n",
      "941: loss: 0.0643767174333334\n",
      "942: loss: 0.09822243452072144\n",
      "943: loss: 0.045856133103370667\n",
      "944: loss: 0.04873087164014578\n",
      "945: loss: 0.1259515369310975\n",
      "946: loss: 0.060075560584664345\n",
      "947: loss: 0.10236478224396706\n",
      "948: loss: 0.07656134106218815\n",
      "949: loss: 0.03192082606256008\n",
      "950: loss: 0.09253873024135828\n",
      "951: loss: 0.1385194156318903\n",
      "952: loss: 0.1118546724319458\n",
      "953: loss: 0.019722359254956245\n",
      "953: saving model with minimum loss to results\n",
      "954: loss: 0.10535908862948418\n",
      "955: loss: 0.1245589442551136\n",
      "956: loss: 0.05899715516716242\n",
      "957: loss: 0.11034597456455231\n",
      "958: loss: 0.08031168021261692\n",
      "959: loss: 0.08154482115060091\n",
      "960: loss: 0.10201863199472427\n",
      "961: loss: 0.05368203856050968\n",
      "962: loss: 0.0651196800172329\n",
      "963: loss: 0.0500126127153635\n",
      "964: loss: 0.05714232288300991\n",
      "965: loss: 0.036677462980151176\n",
      "966: loss: 0.06275610998272896\n",
      "967: loss: 0.09503639675676823\n",
      "968: loss: 0.0885984692722559\n",
      "969: loss: 0.04456344619393349\n",
      "970: loss: 0.07699435576796532\n",
      "971: loss: 0.09381552785634995\n",
      "972: loss: 0.026230940595269203\n",
      "973: loss: 0.1152248214930296\n",
      "974: loss: 0.06088215671479702\n",
      "975: loss: 0.028971830382943153\n",
      "976: loss: 0.06790014542639256\n",
      "977: loss: 0.025077635422348976\n",
      "978: loss: 0.053585490211844444\n",
      "979: loss: 0.07061631977558136\n",
      "980: loss: 0.0950763700529933\n",
      "981: loss: 0.07351179420948029\n",
      "982: loss: 0.0514516644179821\n",
      "983: loss: 0.08489361219108105\n",
      "984: loss: 0.06082016043365002\n",
      "985: loss: 0.09511754661798477\n",
      "986: loss: 0.03900604136288166\n",
      "987: loss: 0.0756728919222951\n",
      "988: loss: 0.04050968773663044\n",
      "989: loss: 0.06458725687116385\n",
      "990: loss: 0.07606888934969902\n",
      "991: loss: 0.16957995668053627\n",
      "992: loss: 0.11019559390842915\n",
      "993: loss: 0.034562721848487854\n",
      "994: loss: 0.08692396059632301\n",
      "995: loss: 0.072030127979815\n",
      "996: loss: 0.0765894167125225\n",
      "997: loss: 0.041539642959833145\n",
      "998: loss: 0.11362653784453869\n",
      "999: loss: 0.0695342943072319\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "mulan_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved mulan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulan_trainer = MuLaNTrainer(mulan=mulan, dataset=training_data, num_train_steps=1000, batch_size=2, grad_accum_every=16)\n",
    "min_loss_path = '/root/musiclm-pytorch/results/mulan_min_loss.pt'\n",
    "mulan_trainer.load(min_loss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most sim music for the given text description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "special_characters = {'&', ',', '\"', \"'\", '/', ';', '', '(', '', '', '.', ')', '-', '\\n', ':'}\n",
    "def replace_special_characters_with_space(text):\n",
    "    for char in special_characters:\n",
    "        text = text.replace(char, ' ')\n",
    "    return text\n",
    "# input\n",
    "query_text = ['This music features a classic piano solo, showcasing intricate melodies and expressive harmonies. The timeless elegance and nuanced performance create an immersive and captivating listening experience.']\n",
    "\n",
    "query_text = [replace_special_characters_with_space(text) for text in query_text]\n",
    "\n",
    "\n",
    "# get the latent representation of the query text\n",
    "query_text_latent = mulan_trainer.mulan.get_text_latents(raw_texts=query_text)\n",
    "\n",
    "# get the audio representation of the query text, highest similarity, iterate over all mulan dataset\n",
    "max_similarity = 0\n",
    "max_similarity_idx = 0\n",
    "idx_simliarity_text_list = []\n",
    "for idx in tqdm(range(len(training_data))):\n",
    "    wav, txt = training_data[idx]\n",
    "    # append fake batch\n",
    "    wav = torch.unsqueeze(wav, 0).to(mulan_trainer.device)\n",
    "    audio_latent = mulan_trainer.mulan.get_audio_latents(wav)\n",
    "    \n",
    "    # compute cosine similarity between two latents\n",
    "    similarity = torch.nn.functional.cosine_similarity(query_text_latent, audio_latent).detach().cpu().numpy()\n",
    "    \n",
    "    idx_simliarity_text_list.append((idx, similarity, txt))\n",
    "    \n",
    "    if similarity > max_similarity:\n",
    "        max_similarity = similarity\n",
    "        max_similarity_idx = idx\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from IPython.display import display\n",
    "\n",
    "# sort the similarity_text list by similarity (higest to lowest)\n",
    "print(query_text)\n",
    "idx_simliarity_text_list.sort(key=lambda x: x[1], reverse=True)\n",
    "for sim_text in idx_simliarity_text_list[:10]:\n",
    "    idx, sim, txt = sim_text\n",
    "    print(f'{idx} {sim} - {txt}')\n",
    "    # display(ipd.Audio(training_data[idx][0], rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the best match\n",
    "print(training_data[max_similarity_idx][1])\n",
    "# play the audio of the best match\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(training_data[max_similarity_idx][0], rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "text1 = ['his voice.  song']\n",
    "text2 = ['his voice   \\n  song']\n",
    "text3 = ['his voice song']\n",
    "\n",
    "embed1 = mulan.get_text_latents(raw_texts=text1)\n",
    "print(embed1.shape)\n",
    "embed2 = mulan.get_text_latents(raw_texts=text2)\n",
    "print(embed2.shape)\n",
    "embed3 = mulan.get_text_latents(raw_texts=text3)\n",
    "\n",
    "print(embed1.sum())\n",
    "print(embed2.sum())\n",
    "print(embed3.sum())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
